# Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the CC BY-NC 4.0 license [see LICENSE for details].

import gc
import random
import warnings
from typing import List

import numpy as np
import PIL
import torch
from PIL import Image
from qwen_vl_utils import process_vision_info
from torch import nn
from transformers import AutoConfig, LogitsProcessor, Qwen2_5_VLProcessor
from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import \
    Qwen2_5_VLForConditionalGeneration

import rv_train.constants as C
from rv_train.utils.train_utils import ForkedPdb as debug  # noqa: F401


class NumberSpaceOnlyProcessor(LogitsProcessor):
    """
    Logits processor that constrains generation to only numbers (0-9), spaces, and end-of-text tokens.
    å¼ºåˆ¶ Qwen è¾“å‡º â€œç©ºæ ¼åˆ†éš”çš„æ•´æ•°åºåˆ—â€ï¼Œæ–¹ä¾¿åé¢è§£ç ä¸ºåŠ¨ä½œ
    å…¸å‹çš„ã€ŒæŠŠ LLM è¾“å‡ºå½“æˆ structured action çš„æŠ€å·§ã€
    """

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # Get token IDs for allowed tokens
        self.allowed_tokens = set()

        # Add numbers 0-9
        for i in range(10):
            # å¼€å¤´ã€ç»“å°¾ç­‰ç‰¹æ®Šç¬¦å·ä¸è¦ï¼›è¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
            token_id = tokenizer.encode(str(i), add_special_tokens=False)[0]
            self.allowed_tokens.add(token_id)
        # Add space token
        space_token_id = tokenizer.encode(" ", add_special_tokens=False)[0]
        self.allowed_tokens.add(space_token_id)

        # Add end of text token
        # æ²¡æœ‰ EOS tokenï¼Œæ¨¡å‹ä¼šä¸€ç›´ç”Ÿæˆç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦
        if tokenizer.eos_token_id is not None:
            self.allowed_tokens.add(tokenizer.eos_token_id)

    def __call__(self, input_ids, scores):
        # Set logits to negative infinity for all tokens except allowed ones
        # input_ids: å½“å‰å·²ç”Ÿæˆçš„tokenåºåˆ— [batch_size, sequence_length]
        # scores: æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹åˆ†æ•° [batch_size, vocab_size]
        mask = torch.full_like(scores, float("-inf"))
        for token_id in self.allowed_tokens:
            mask[:, token_id] = 0
        return scores + mask


def format_data(system_message, image, instr, action_txt):
    """
    Convert the data into the format required by the model
    :param system_message: str, the system message
    :param image: list of PIL images, the image to be processed
    :param instr: str, the instruction
    :param action_txt: str, the action text
    :return: list of dicts, the format required by the model
    system: ç³»ç»Ÿè§’è‰²è®¾å®š
    user: ç”¨æˆ·è¾“å…¥ï¼ŒåŒ…å«ï¼š
        å›¾åƒï¼ˆä¸€å¼ æˆ–å¤šå¼ ï¼‰
        æ–‡æœ¬æŒ‡ä»¤
    assistant: åŠ©æ‰‹å›å¤ï¼ˆé€šå¸¸æ˜¯åŠ¨ä½œæŒ‡ä»¤ï¼‰
    """
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": system_message}],
        },
        {
            "role": "user",
            "content": [{"type": "image", "image": _image} for _image in image]
            + [{"type": "text", "text": instr}],
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": action_txt}],
        },
    ]


class QwenActor(nn.Module):
    def __init__(
        self,
        qwen_model_id,
        action_type,
        original_action_dim,
        horizon,
        history,
        use_lora=True,
        use_qlora=True,
        num_cam=1,
        lora_config="",
        lora_rank=8,
        rgb_input=False,
        rgb_img_size=(84, 84),
        add_vision_id=False,
        tiled_rgb_imgs=False,
        num_bins_actions=1000,
        use_flash_attention_2=False,
        system_message_version=1,
        action_mask_aug=0,
        action_mask_aug_per=0.1,
        attention_dropout=0.0,
    ):
        """
        :param qwen_model_id: str, the id of the qwen model to use
        :param action_type: str, the type of action to use, either ORIGINAL or EE
        :param original_action_dim: int, the dimension of the original action
        :param horizon: int, the horizon of the action
        :param history: int, the history of the action
        :param use_qlora: bool, whether to use qlora for parameter efficient fine-tuning
        :param num_cam: int, the number of cameras for rgb input
        :param lora_config: str, the lora configuration to use, either empty string or "default"
        :param lora_rank: int, the rank of the lora to use, only used if lora_config is "default"
        :param rgb_input: bool, whether to use rgb image input
        :param rgb_img_size: tuple, the size of the rgb image input (height, width)
        :param add_vision_id: bool, whether to add vision id to the input for qwen2.5
        :param tiled_rgb_imgs: bool, whether to tile the rgb images into a single image instead feeding them separately
        :param num_bins_actions: int, the number of bins in which each action dimension is discretized
        :param use_flash_attention_2: bool, whether to use flash attention 2 for faster training and inference
        :param attention_dropout: float, the dropout rate for the attention layer in the qwen model. Only tested when use_lora is False.
        :param qwen_model_id: str, è¦ä½¿ç”¨çš„Qwenæ¨¡å‹ID
        :param action_type: str, è¦ä½¿ç”¨çš„åŠ¨ä½œç±»å‹ï¼Œå¯ä»¥æ˜¯ORIGINALæˆ–EE
        :param original_action_dim: int, åŸå§‹åŠ¨ä½œçš„ç»´åº¦
        :param horizon: int, åŠ¨ä½œçš„æ—¶é—´æ­¥é•¿ï¼ˆé¢„æµ‹èŒƒå›´ï¼‰
        :param history: int, åŠ¨ä½œçš„å†å²é•¿åº¦
        :param use_qlora: bool, æ˜¯å¦ä½¿ç”¨QLoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
        :param num_cam: int, RGBè¾“å…¥çš„æ‘„åƒå¤´æ•°é‡
        :param lora_config: str, è¦ä½¿ç”¨çš„LoRAé…ç½®ï¼Œå¯ä»¥æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–"default"
        :param lora_rank: int, è¦ä½¿ç”¨çš„LoRAç§©ï¼Œä»…åœ¨lora_configä¸º"default"æ—¶ä½¿ç”¨
        :param rgb_input: bool, æ˜¯å¦ä½¿ç”¨RGBå›¾åƒè¾“å…¥
        :param rgb_img_size: tuple, RGBå›¾åƒè¾“å…¥çš„å°ºå¯¸ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰
        :param add_vision_id: bool, æ˜¯å¦ä¸ºQwen2.5è¾“å…¥æ·»åŠ è§†è§‰ID
        :param tiled_rgb_imgs: bool, æ˜¯å¦å°†RGBå›¾åƒæ‹¼æ¥æˆå•ä¸ªå›¾åƒè€Œä¸æ˜¯åˆ†åˆ«è¾“å…¥
        :param num_bins_actions: int, æ¯ä¸ªåŠ¨ä½œç»´åº¦ç¦»æ•£åŒ–çš„åˆ†ç®±æ•°é‡
        :param use_flash_attention_2: bool, æ˜¯å¦ä½¿ç”¨Flash Attention 2ä»¥åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
        :param attention_dropout: float, Qwenæ¨¡å‹ä¸­æ³¨æ„åŠ›å±‚çš„dropoutç‡ã€‚ä»…åœ¨use_loraä¸ºFalseæ—¶ç»è¿‡æµ‹è¯•ã€‚
        """
        super(QwenActor, self).__init__()

        # current assumptions
        if use_qlora:
            assert use_lora, "use_lora must be True if use_qlora is True"
        # LoRA ä¸ attention dropout æ²¡æœ‰æ ¹æœ¬å†²çªï¼›ä¿å®ˆï¼›ä½œè€…æ²¡æœ‰æµ‹è¯•
        if attention_dropout > 0.0:
            assert (
                not use_lora
            ), "attention_dropout is only supported when use_lora is False"
        assert lora_config in ["", "default"]
        if history > 1 or num_cam > 1:
            assert (
                add_vision_id
            ), "add_vision_id must be True if history > 1 or num_cam > 1"

        # assert not use_flash_attention_2, "use_flash_attention_2 is not supported yet, it requires tokenizer.pad=left which we have not fully implemented/understood"

        # for Qwen model, we need to load the parameters before DDP
        # in case we want to load the model from a checkpoint
        """
        # æ­£ç¡®çš„æ–¹å¼ï¼šåœ¨DDPä¹‹å‰åŠ è½½æ£€æŸ¥ç‚¹
        model = QwenActor(...)                    # åˆå§‹åŒ–éšæœºæƒé‡
        model.load_state_dict(checkpoint)        # æ‰€æœ‰è¿›ç¨‹åŠ è½½ç›¸åŒæ£€æŸ¥ç‚¹
        model = DDP(model)                       # DDPåŒæ­¥æ£€æŸ¥ç‚¹æƒé‡

        # ç»“æœï¼šæ‰€æœ‰GPUä¸Šçš„æ¨¡å‹å‚æ•°ä¸€è‡´ï¼
        # GPU0: æ£€æŸ¥ç‚¹æƒé‡
        # GPU1: æ£€æŸ¥ç‚¹æƒé‡
        # GPU2: æ£€æŸ¥ç‚¹æƒé‡
        """
        self.load_param_before_ddp = True

        self.qwen_model_id = qwen_model_id
        self.action_type = action_type
        self.original_action_dim = original_action_dim
        self.horizon = horizon
        self.history = history
        self.use_lora = use_lora
        self.use_qlora = use_qlora
        self.num_cam = num_cam
        self.lora_config = lora_config
        self.lora_rank = lora_rank
        self.rgb_input = rgb_input
        self.rgb_img_size = rgb_img_size
        self.add_vision_id = add_vision_id
        self.tiled_rgb_imgs = tiled_rgb_imgs
        self.num_bins_actions = num_bins_actions
        self.use_flash_attention_2 = use_flash_attention_2
        self.action_mask_aug_per = action_mask_aug_per
        self.attention_dropout = attention_dropout

        self.model = self.load_qwen_model(
            qwen_model_id=self.qwen_model_id,
            use_lora=self.use_lora,
            use_qlora=self.use_qlora,
            lora_config=self.lora_config,
            lora_rank=self.lora_rank,
            use_flash_attention_2=self.use_flash_attention_2,
            attention_dropout=self.attention_dropout,
        )
        """
        # æ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§æ—¶é—´æ¢ç©ºé—´çš„æŠ€æœ¯
        # é—®é¢˜ï¼šå¤§æ¨¡å‹è®­ç»ƒæ—¶GPUå†…å­˜ä¸è¶³
        # è§£å†³æ–¹æ¡ˆï¼šä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»ï¼Œè€Œæ˜¯åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
        """
        # Enable gradient checkpointing if requested å¹¶æ²¡æœ‰å®ç°
        """
        input_ids =    [1, 2, 3, 4, 5, 6]      # æ¨¡å‹è¾“å…¥
        labels =       [2, 3, 4, 5, 6, -100]   # è®­ç»ƒç›®æ ‡ï¼Œæœ€åä¸€ä¸ªä½ç½®å¿½ç•¥
        # å°†ä¸éœ€è¦è®¡ç®—æŸå¤±çš„ä½ç½®è®¾ä¸º-100ï¼š
        """
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)

        # self.rgb_img_size = (84, 84)
        # self.min_pixel = self.max_pixel = 84 * 84  # = 7056åƒç´ 
        self.min_pixel = self.max_pixel = self.rgb_img_size[0] * self.rgb_img_size[1]
        if self.rgb_input and self.tiled_rgb_imgs:
            self.min_pixel *= self.history * self.num_cam
            self.max_pixel *= self.history * self.num_cam

        """
        å¯¹ è‡ªå›å½’ LMï¼ˆQwen è¿™ç§ï¼‰ æ¥è¯´ï¼š
        è®­ç»ƒæ—¶å¤šæ•°äººç”¨ right paddingï¼ˆæ–¹ä¾¿ maskï¼‰ã€‚
        æ‰¹é‡ç”Ÿæˆ + KV cache / flash attention æ—¶ï¼Œé€šå¸¸ä¼šæ¨èç”¨ left paddingï¼Œå› ä¸ºæœ€åä¸€ä¸ªçœŸå® token éƒ½åœ¨åºåˆ—æœ«å°¾ï¼Œbatch é‡Œå¯¹é½å¾—æ›´æ•´é½ï¼Œè®¡ç®—æ›´é«˜æ•ˆã€‚
        """
        self.processor = self.load_qwen_model_processor(
            qwen_model_id=qwen_model_id,
            min_pixel=self.min_pixel,
            max_pixel=self.max_pixel,
            padding_side="left" if use_flash_attention_2 else None,
        )
        # é»˜è®¤èŒƒå¼ = ä¸€å¥— tokenizer + ä¸€å¥— vocabï¼ŒåŒæ—¶çº¦æŸè¾“å…¥çš„ç¼–ç æ–¹å¼å’Œè¾“å‡ºçš„å¯é€‰ token ç©ºé—´ã€‚
        # å®ƒä¸ä¼šå»æ”¹åŸæ¥çš„ processorï¼Œä¸ä¼šå»ä¿®æ”¹ tokenizer çš„ vocab æˆ–é…ç½®ã€‚
        self.logits_processor = NumberSpaceOnlyProcessor(self.processor.tokenizer)

        """
        DP3 = ä¸€ç§é€šç”¨çš„æ•°æ®æ ¼å¼ï¼ˆåŒ…æ‹¬ obs/action çš„ç»“æ„ + stats.jsonï¼‰ã€‚
        è€Œæ¯ä¸ªæ•°æ®é›†è‡ªå·±çš„ state = è¿™ä¸ªæ ¼å¼é‡Œé¢ obs çš„å…·ä½“å†…å®¹ï¼ˆå› æ•°æ®é›†è€Œå¼‚ï¼‰ã€‚
        â€œDP3 æ•°æ®é›†çš„ statsâ€ æŒ‡çš„æ˜¯æ ¼å¼ä¸­çš„ç»Ÿè®¡é‡ï¼Œä¸æ˜¯ç‰¹å®šä»»åŠ¡/ç¯å¢ƒçš„ stateã€‚

        æ²¡æœ‰ä» DP3 æ ¼å¼çš„ meta/stats.json è¯»å– mean/stdï¼Œ
        è€Œæ˜¯å†™æ­»åœ¨ä»£ç é‡Œçš„å›ºå®š normalization å‚æ•°ã€‚
        """
        # ç°åœ¨åªæ˜¯æš‚æ—¶ç”¨å†™æ­»çš„ DP3 æ•°æ®é›†å½’ä¸€åŒ–å‚æ•°ï¼Œä¸‹æ¬¡è¯·æ”¹æˆä» stats æ–‡ä»¶é‡Œè¯»ï¼Œåˆ«ä¸€ç›´ç”¨è¿™å—ç¡¬ç¼–ç ã€‚
        print(
            "WARNING: Using hardcoded dataset stats for DP3. This should be replaced with loading from a file."
        )

        """
        å› ä¸ºå®é™…æ•°æ®é›†ç‰¹åˆ«æ‚ï¼š
        æœ‰çš„åŠ¨ä½œæ˜¯ 7 ç»´ï¼ˆEEï¼‰
        æœ‰çš„æ˜¯ 8 ç»´ï¼ˆé¢å¤–åŠ  gripper one-hotï¼‰
        æœ‰çš„æ˜¯ 12 æˆ– 13 ç»´ï¼ˆå§¿æ€ç”¨ 6D ç‰¹å¾ï¼‰
        æœ‰äº›æ˜¯ joint-spaceï¼ˆå…³èŠ‚è§’å¢é‡ï¼‰
        æœ‰äº›æ˜¯äºŒé˜¶æ®µåŠ¨ä½œï¼ˆopen/close + xyzï¼‰
        å¦‚æœæ¨¡å‹ç»Ÿä¸€ä½¿ç”¨æ•°æ®é›†åŠ¨ä½œç»´åº¦ â†’ åŠ¨ä½œå¤´å¿…é¡»æ”¯æŒæ¯ä¸€ç§ç»´åº¦ï¼Œéº»çƒ¦ä¸”éš¾æ³›åŒ–ã€‚

        æ‰€ä»¥ä½œè€…æä¾›ä¸¤æ¡è·¯å¾„ï¼š
        âœ” ORIGINAL â†’ ç”¨æ•°æ®é›†åŠ¨ä½œï¼ˆåŸæ ·ï¼‰
        âœ” EE â†’ å¼ºåˆ¶ç»Ÿä¸€æˆä¸€ä¸ª 7 ç»´ VLA åŠ¨ä½œå¤´ï¼ˆæ›´é€‚åˆå¤šä»»åŠ¡ / å¤šæ•°æ®é›†ï¼‰
        è®ºæ–‡ä¸­çš„â€œåŸå§‹åŠ¨ä½œ(original)â€é€šå¸¸å°±æ˜¯ EE åŠ¨ä½œã€‚
        ä»£ç ä¸­çš„ ORIGINAL æŒ‡çš„æ˜¯â€œç”¨æ•°æ®é›†æœ¬æ¥çš„åŠ¨ä½œæ ¼å¼â€ï¼Œ
        è€Œ EE æ˜¯ä½œè€…è‡ªå·±å®šä¹‰çš„ç»Ÿä¸€ 7D EE è§„èŒƒï¼Œä¸æ•°æ®é›†åŸå§‹æ ¼å¼ä¸ä¸€å®šä¸€è‡´ã€‚
        """
        if action_type == C.ORIGINAL:
            self.act_dim = original_action_dim
        elif action_type == C.EE:
            self.act_dim = 7
        else:
            assert False

        """
        "åˆ†æè¾“å…¥å›¾åƒå¹¶é¢„æµ‹æ¥ä¸‹æ¥ {self.horizon} ä¸ªæ—¶é—´æ­¥çš„æœºå™¨äººåŠ¨ä½œã€‚
        æ¯ä¸ªåŠ¨ä½œæœ‰ {self.act_dim} ä¸ªç»´åº¦ã€‚
        è¾“å‡ºä¸€ä¸ªåŒ…å« {self.horizon * self.act_dim} ä¸ªæ•´æ•°ï¼ˆæ¯ä¸ªæ•´æ•°èŒƒå›´ 0-{self.num_bins_actions}ï¼‰çš„å•ä¸€åºåˆ—ï¼Œ
        æŒ‰é¡ºåºè¡¨ç¤º {self.horizon} ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œã€‚åªæä¾›ç©ºæ ¼åˆ†éš”çš„æ•°å­—ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ã€‚"
        """
        self.system_message = f"Analyze the input image and predict robot actions for the next {self.horizon} timesteps. Each action has {self.act_dim} dimensions. Output a single sequence of {self.horizon * self.act_dim} integers (0-{self.num_bins_actions} each), representing the {self.horizon} timesteps sequentially. Provide only space separated numbers. Nothing else."

        # todo: need better way to determine it
        self.num_tokens = 1024
        # è¿™æ˜¯ç”¨æ¥è£…**â€œæ•°æ®é›†åŸå§‹çš„ç»Ÿè®¡é‡ï¼ˆmean/std ç­‰ï¼‰â€**çš„ï¼Œæ ¼å¼è·Ÿæ•°æ®é›†è‡ªå¸¦çš„ä¸€æ¨¡ä¸€æ ·ã€‚
        self.original_dataset_stats = None  # original dataset stats has the same format as the one provided by the dataset
        # æŠŠåŸå§‹æ•°æ®é›†çš„ stats é‡æ–°æ•´ç†æˆ è¿™ä¸ªæ¨¡å‹èƒ½ç›´æ¥ç”¨çš„å½¢å¼ã€‚
        self.dataset_stats = (
            None  # dataset stats is in the format specific to the model
        )

        """
        self._sysuser_lenï¼šç”¨æ¥è®°å½• system+user å‰ç¼€çš„ token æ•°ï¼ˆç¼“å­˜ç”¨ï¼‰
        self.cache_sysuser_lenï¼šå†³å®šè¦ä¸è¦å¤ç”¨è¿™ä¸ªé•¿åº¦ï¼ˆå‡è®¾æ¯æ¬¡ prompt æ¨¡æ¿ç›¸åŒï¼‰ï¼Œä»è€Œæ–¹ä¾¿ï¼š
        ç»™ prefix éƒ¨åˆ†è®¾ labels=-100
        æˆªå– â€œä»è¿™é‡Œå¾€åçš„ token æ˜¯åŠ¨ä½œåºåˆ—â€ã€‚
        """
        self._sysuser_len = None
        self.cache_sysuser_len = False

    def set_dataset_stats(self, dataset_stats):
        """
        Set the dataset stats for the model
        :param dataset_stats: dict, the dataset stats
        """
        if dataset_stats == {}:
            # â€œDataset stats æ˜¯ç©ºçš„ï¼Œå¾ˆå¯èƒ½è¿™å°æœºå™¨ä¸Šæ²¡æœ‰ç”¨æ¥ç®— stats çš„æ•°æ®ã€‚å¦‚æœä½ åªæ˜¯åŠ è½½ä¸€ä¸ªå·²ç»é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥å¿½ç•¥ã€‚â€
            # åå½’ä¸€åŒ–ä½œè€…ä¹Ÿè®¸æ”¾åˆ°åˆ«çš„åœ°æ–¹
            warnings.warn(
                "Dataset stats is empty likely because the system does not have the data used to compute the stats. Ignore this is you are loading a pretrained model."
            )
            return

        self.original_dataset_stats = dataset_stats
        if self.action_type == C.ORIGINAL:
            self.dataset_stats = dataset_stats["out_ori_act"]
        else:
            # ç›®å‰è¿˜æ²¡å®ç°â€œEE æ¨¡å¼ä¸‹è¯¥æ€ä¹ˆä»åŸå§‹ dataset_stats é‡Œæ„é€ ä¸€ä»½é€‚é… 7 ç»´åŠ¨ä½œçš„ statsâ€ã€‚
            raise NotImplementedError(f"Action type {self.action_type} not implemented")

    @staticmethod
    def load_qwen_model(
        qwen_model_id,  # æ¨¡å‹IDæˆ–è·¯å¾„ï¼Œå¦‚ "Qwen/Qwen2.5-VL-7B-Instruct"
        use_lora,
        use_qlora,
        lora_config,
        lora_rank,
        use_flash_attention_2,
        attention_dropout,  # # æ³¨æ„åŠ›dropoutç‡
    ):
        if lora_config == "":
            lora_config = None
        elif lora_config == "default":
            if use_lora:
                """
                åŸå§‹å‰å‘ä¼ æ’­: h = Wx
                LoRAå‰å‘ä¼ æ’­: h = Wx + BAx

                Rank
                # æ§åˆ¶é€‚é…å™¨çš„è¡¨è¾¾èƒ½åŠ›
                r=8   # è¾ƒå°ç§©ï¼Œå‚æ•°å°‘ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
                r=16  # ä¸­ç­‰ç§©ï¼Œå¹³è¡¡æ•ˆæœå’Œæ•ˆç‡
                r=64  # è¾ƒå¤§ç§©ï¼Œå‚æ•°å¤šï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

                lora_alpha
                # æ§åˆ¶LoRAè¾“å‡ºçš„ç¼©æ”¾ç¨‹åº¦
                å®é™…è¾“å‡º = (BAx) * (alpha / r)
                ç±»ä¼¼äºå­¦ä¹ ç‡ï¼Œæ§åˆ¶é€‚é…å™¨å¯¹åŸå§‹è¾“å‡ºçš„å½±å“ç¨‹åº¦;é€šå¸¸è®¾ç½®ä¸º r çš„ 1-2 å€;alpha/r æ¯”å€¼è¶Šå¤§ï¼ŒLoRAå½±å“è¶Šå¼º

                target
                # æŒ‡å®šå“ªäº›å±‚åº”ç”¨LoRA
                target_modules=["q_proj", "v_proj"]  # åªå¯¹queryå’ŒvalueæŠ•å½±åº”ç”¨
                # æˆ–è€…
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # æ‰€æœ‰æ³¨æ„åŠ›æŠ•å½±
                # æˆ–è€…
                target_modules="all-linear"  # æ‰€æœ‰çº¿æ€§å±‚

                type
                æ©ç å’ŒæŸå¤±è®¡ç®—ä¸åŒ
                task_type="CAUSAL_LM"        # å› æœè¯­è¨€å»ºæ¨¡ (æ–‡æœ¬ç”Ÿæˆ)
                task_type="SEQ_2_SEQ_LM"     # åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
                task_type="TOKEN_CLS"        # ä»¤ç‰Œåˆ†ç±»
                task_type="QUESTION_ANS"     # é—®ç­”ä»»åŠ¡


                """
                from peft import LoraConfig, get_peft_model

                lora_config = LoraConfig(
                    lora_alpha=16,  # LoRAç¼©æ”¾å› å­
                    lora_dropout=0.05,  # LoRAå±‚çš„dropout
                    r=lora_rank,  # LoRAç§©ï¼Œæ§åˆ¶å‚æ•°é‡
                    bias="none",  # ä¸è®­ç»ƒä»»ä½•åç½® (æœ€å¸¸ç”¨);æœ€èŠ‚çœå‚æ•°ï¼Œæ•ˆæœé€šå¸¸è¶³å¤Ÿå¥½
                    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ¨¡å—
                    task_type="CAUSAL_LM",  # å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡
                )
        else:
            raise ValueError(f"Invalid lora_config: {lora_config}")

        # Qwen Init BitsAndBytes Configï¼ˆæ¯”ç‰¹ä¸å­—èŠ‚é…ç½®ï¼‰
        bnb_config = None
        if use_lora and use_qlora:
            # QLoRA = é‡åŒ–ï¼ˆQuantizationï¼‰ + LoRA
            from transformers import BitsAndBytesConfig

            """
                # NF4 (Normal Float 4)
                # - ä¸“é—¨ä¸ºæ­£æ€åˆ†å¸ƒæ•°æ®ä¼˜åŒ–
                # - åœ¨é‡è¦åŒºåŸŸï¼ˆæ¥è¿‘0ï¼‰æœ‰æ›´é«˜ç²¾åº¦
                # - é€‚åˆç¥ç»ç½‘ç»œæƒé‡
                # FP4 (Float Point 4)
                # - å‡åŒ€ç²¾åº¦åˆ†å¸ƒ
                # - é€‚åˆé€šç”¨æ•°æ®
                # å®é™…æ•ˆæœï¼šNF4åœ¨ç›¸åŒbitæ•°ä¸‹ç²¾åº¦æŸå¤±æ›´å°

                # ç¬¬ä¸€æ¬¡é‡åŒ–ï¼šåŸå§‹æƒé‡ â†’ 4bitå€¼ + ç¼©æ”¾å› å­
                original_weights = [0.123, 0.456, 0.789, 0.234, ...]
                quantized_values = [1, 3, 5, 2, ...]        # 4bitå€¼
                scale_factors = [0.1, 0.2, ...]             # ç¼©æ”¾å› å­
                # ç¬¬äºŒæ¬¡é‡åŒ–ï¼šå¯¹ç¼©æ”¾å› å­æœ¬èº«å†æ¬¡é‡åŒ–
                # scale_factors â†’ è¿›ä¸€æ­¥å‹ç¼©
                # æ•ˆæœï¼šé¢å¤–èŠ‚çœçº¦0.4-0.5bit/å‚æ•°
                ç¼©æ”¾å› å­ = (35 - (-25)) / 15 = 60 / 15 = 4
                é‡åŒ–è¿‡ç¨‹ï¼š
                åŒ—äº¬ï¼š-10Â°C â†’ (-10 - (-25)) / 4 = 15/4 â‰ˆ 4ï¼ˆåˆ»åº¦å€¼ï¼‰
                å¹¿å·ï¼š25Â°C  â†’ (25 - (-25)) / 4 = 50/4 â‰ˆ 13ï¼ˆåˆ»åº¦å€¼ï¼‰
                å“ˆå°”æ»¨ï¼š-25Â°C â†’ (-25 - (-25)) / 4 = 0ï¼ˆåˆ»åº¦å€¼ï¼‰
                ä¸‰äºšï¼š35Â°C  â†’ (35 - (-25)) / 4 = 60/4 = 15ï¼ˆåˆ»åº¦å€¼ï¼‰
                å­˜å‚¨ï¼š
                åˆ»åº¦å€¼ï¼š[4, 13, 0, 15]ï¼ˆ4bitï¼‰
                ç¼©æ”¾å› å­ï¼š4ï¼ˆä¿å­˜è¿™ä¸ªå°±èƒ½è¿˜åŸï¼‰
                æœ€å°å€¼ï¼š-25ï¼ˆä¿å­˜è¿™ä¸ªç¡®å®šèµ·ç‚¹ï¼‰

                # bfloat16ä¼˜åŠ¿ï¼š
                # - ä¿æŒä¸float32ç›¸åŒçš„æŒ‡æ•°èŒƒå›´ï¼ˆ8bitæŒ‡æ•°ï¼‰
                # - å‡å°‘å°¾æ•°ç²¾åº¦ï¼ˆ7bitå°¾æ•°ï¼‰
                # - è®­ç»ƒç¨³å®šæ€§æ›´å¥½ï¼Œå‡å°‘æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
            """
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,  # æ ¸å¿ƒï¼š4bité‡åŒ– å°†æ¨¡å‹æƒé‡ä»16bit/32bitå‹ç¼©åˆ°4bit å†…å­˜å‡å°‘: 75.0%ï¼ˆ16bitï¼‰
                bnb_4bit_use_double_quant=True,  # åŒé‡åŒ–ï¼Œè¿›ä¸€æ­¥å‹ç¼©
                bnb_4bit_quant_type="nf4",  # é‡åŒ–æ•°æ®ç±»å‹
                bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—ç²¾åº¦ å­˜å‚¨ä¸è®¡ç®—æ˜¯ä¸åŒçš„
            )

        extra_kwargs = {}
        if use_flash_attention_2:
            """
            # ä¼ ç»Ÿæ³¨æ„åŠ› vs Flash Attention 2
            ä¼ ç»Ÿæ³¨æ„åŠ›: O(NÂ²) å¤æ‚åº¦ï¼Œé€Ÿåº¦æ…¢
            Flash Attention 2: ä¼˜åŒ–åˆ°æ¥è¿‘ O(N)ï¼Œé€Ÿåº¦å¿«2-4å€
            # å†…å­˜å ç”¨å¯¹æ¯”ï¼ˆåºåˆ—é•¿åº¦=4096ï¼‰
            ä¼ ç»Ÿæ³¨æ„åŠ›: ~16GB
            Flash Attention 2: ~4GB  # å‡å°‘75%å†…å­˜

            æ ¸å¿ƒï¼šåˆ†å—è®¡ç®—å°†å¤§çŸ©é˜µåˆ†æˆå°å—å¤„ç†
            """
            extra_kwargs["attn_implementation"] = "flash_attention_2"

        if attention_dropout > 0.0:
            """
            # é¢„è®­ç»ƒæ—¶é€šå¸¸ç”¨å¾ˆå°çš„dropout
            attention_dropout = 0.0  # æˆ– 0.1
            # åŸå› ï¼šé¢„è®­ç»ƒæ•°æ®é‡å¤§ï¼Œè¿‡æ‹Ÿåˆé£é™©å°
            # å¾®è°ƒæ—¶æ•°æ®é‡å°ï¼Œéœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
            attention_dropout = 0.1  # æˆ– 0.2
            # åŸå› ï¼šé˜²æ­¢åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ
            """
            config = AutoConfig.from_pretrained(qwen_model_id)
            config.attention_dropout = attention_dropout
            extra_kwargs["config"] = config

        """
        # è¿™ä¸‰ç§å†™æ³•æ•ˆæœç›¸åŒ å¯¹äºå¤§å¤šæ•°æƒ…å†µ ä¸æŒ‡å®š device_mapï¼Œè®©åº“è‡ªåŠ¨å¤„ç† éœ€è¦ç²¾ç¡®æ§åˆ¶æ—¶å†å†™
        device_map={"": "cuda:0"}
        device_map="cuda:0"  # ç®€åŒ–å†™æ³•
        device_map=0         # æ›´ç®€åŒ–çš„å†™æ³•
        """
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            qwen_model_id,
            # device_map={"": "cuda:0"},  # Use the explicit map
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
            **extra_kwargs,
        )

        if use_lora and (lora_config is not None):
            model = get_peft_model(model, lora_config)

        return model

    @staticmethod
    def load_qwen_model_processor(
        qwen_model_id,  # æ¨¡å‹IDï¼Œå¦‚ "Qwen/Qwen2.5-VL-7B-Instruct"
        min_pixel,  # å›¾åƒæœ€å°åƒç´ å€¼ï¼ˆé¢„å¤„ç†ç”¨ï¼‰ åŸå§‹åƒç´ å€¼ â†’ ç¼©æ”¾åˆ° [min_pixel, max_pixel] èŒƒå›´
        max_pixel,  # å›¾åƒæœ€å¤§åƒç´ å€¼ï¼ˆé¢„å¤„ç†ç”¨ï¼‰ åŸå§‹åƒç´ å€¼ â†’ ç¼©æ”¾åˆ° [min_pixel, max_pixel] èŒƒå›´
        padding_side,  # æ–‡æœ¬å¡«å……æ–¹å‘
    ):
        """
        Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰
            ä»…å¤„ç†æ–‡æœ¬ï¼šæ–‡æœ¬ â†’ tokens
            åŠŸèƒ½å•ä¸€ï¼šåªè´Ÿè´£æ–‡æœ¬åˆ†è¯
        Processorï¼ˆå¤„ç†å™¨ï¼‰:å†…éƒ¨å¤æ‚ç»„æˆ
            å¤šæ¨¡æ€å¤„ç†ï¼šæ–‡æœ¬ + å›¾åƒ â†’ ç»Ÿä¸€æ ¼å¼
            åŠŸèƒ½å…¨é¢ï¼šåè°ƒåˆ†è¯å™¨å’Œå›¾åƒå¤„ç†å™¨

        å¡«å……PAD
        æ–‡æœ¬ç”Ÿæˆ	"right"	ä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œä¾¿äºè‡ªå›å½’ç”Ÿæˆ
        æ–‡æœ¬åˆ†ç±»	"left"	åºåˆ—ç»“å°¾å¯¹é½ï¼Œä¾¿äºåˆ†ç±»ç‰¹å¾æå–
        é—®ç­”ç³»ç»Ÿ	"right"	é—®é¢˜ä¿¡æ¯åœ¨å‰ï¼Œæ¨¡å‹åŸºäºæ­¤ç”Ÿæˆç­”æ¡ˆ
        ç¿»è¯‘ä»»åŠ¡	ç¼–ç å™¨"right"
        è§£ç å™¨"left"	åˆ†åˆ«ä¼˜åŒ–ç¼–ç å’Œè§£ç è¿‡ç¨‹
        """
        if padding_side is not None:
            processor = Qwen2_5_VLProcessor.from_pretrained(
                qwen_model_id,
                min_pixels=min_pixel,
                max_pixels=max_pixel,
                padding_side=padding_side,
            )
        else:
            processor = Qwen2_5_VLProcessor.from_pretrained(
                qwen_model_id,
                min_pixels=min_pixel,
                max_pixels=max_pixel,
            )

        return processor

    def get_min_max_act(self, instruction):
        """
        Get the min and max action for the instruction.
        :param instruction: str, the instruction for the current episode. This is needed for libero bounds 99% as the action space is different for different instructions.
        :return: torch.Tensor, the min and max action.
        è¿™ä¸ªæ–¹æ³•æ ¹æ®ä¸åŒçš„ä»»åŠ¡æŒ‡ä»¤ï¼ˆinstructionï¼‰è¿”å›å¯¹åº”çš„åŠ¨ä½œç©ºé—´çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚
        self.instruction_to_suite = {
            "put the wine bottle on top of the cabinet": "libero_goal",
            "put the banana into the drawer": "libero_object",
            "open the left door of the cabinet": "libero_spatial",
            # ...
        }
        """
        # ç¡®ä¿æŒ‡ä»¤ä¸ä¸ºç©ºï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡æœ‰ä¸åŒçš„åŠ¨ä½œç©ºé—´è¾¹ç•Œã€‚
        assert instruction is not None, "instruction is needed for libero bounds 99%"
        min_act = []
        max_act = []
        for _instruction in instruction:
            _suite = self.instruction_to_suite[_instruction]
            min_act.append(torch.tensor(self.dataset_stats[_suite]["min"]))
            max_act.append(torch.tensor(self.dataset_stats[_suite]["max"]))
        min_act = torch.stack(min_act, dim=0)
        max_act = torch.stack(max_act, dim=0)
        """
        # å‡è®¾æœ‰ä¸‰ä¸ªå½¢çŠ¶ä¸º[2, 3]çš„å¼ é‡
        a = torch.tensor([[1, 2, 3], [4, 5, 6]])
        b = torch.tensor([[7, 8, 9], [10, 11, 12]])
        c = torch.tensor([[13, 14, 15], [16, 17, 18]])

        tensors_to_stack = [a, b, c]  # å…±æœ‰ N=3 ä¸ªå¼ é‡

        # æ²¿ç€ä¸åŒç»´åº¦å †å 
        stacked_dim0 = torch.stack(tensors_to_stack, dim=0)  # å½¢çŠ¶: [3, 2, 3]
        stacked_dim1 = torch.stack(tensors_to_stack, dim=1)  # å½¢çŠ¶: [2, 3, 3]
        stacked_dim2 = torch.stack(tensors_to_stack, dim=2)  # å½¢çŠ¶: [2, 3, 3] (æ­¤ä¾‹ä¸­dim=2æ•ˆæœåŒdim=-1)

        print(f"Original single tensor shape: {a.shape}")     # è¾“å‡º: torch.Size([2, 3])
        print(f"After stack(dim=0) shape: {stacked_dim0.shape}") # è¾“å‡º: torch.Size([3, 2, 3])
        print(f"After stack(dim=1) shape: {stacked_dim1.shape}") # è¾“å‡º: torch.Size([2, 3, 3])
        """
        return min_act, max_act

    def get_text_action(self, actions, instruction=None):
        """
        æ”¯æŒæ‰¹é‡å¤„ç†
        Get the text action from the actions.
        :param actions: torch.Tensor, the actions to convert to text.
        :param instruction: str, the instruction for the current episode. This is needed for libero bounds 99% as the action space is different for different instructions.
        :return: List[str], the text action.
        """
        # TODO: implement this for ee action space ç›®å‰æ˜¯joint

        # ç¡®ä¿åŠ¨ä½œè¾¹ç•Œå¼ é‡ä¸è¾“å…¥åŠ¨ä½œåœ¨ç›¸åŒçš„è®¡ç®—è®¾å¤‡ä¸Šï¼ˆCPU/GPUï¼‰ã€‚
        if (
            (not hasattr(self, "_min_act"))
            or (not hasattr(self, "_max_act"))
            or (self._min_act.device != actions.device)
            or (self._max_act.device != actions.device)
        ):
            # safer to recompute the min and max action for each device
            min_act = torch.tensor(self.dataset_stats["min"], device=actions.device)
            max_act = torch.tensor(self.dataset_stats["max"], device=actions.device)
            self._min_act = min_act
            self._max_act = max_act
        else:
            # use the cached min and max action
            min_act = self._min_act
            max_act = self._max_act

        # ä½œç”¨ï¼šç¡®ä¿æ‰€æœ‰åŠ¨ä½œå€¼éƒ½åœ¨åˆç†çš„ç‰©ç†èŒƒå›´å†…ã€‚
        assert torch.all(min_act <= actions) and torch.all(
            actions <= max_act
        ), f"Action is out of range: {actions}"

        # å°†åŠ¨ä½œä» [min_act, max_act] èŒƒå›´æ˜ å°„åˆ° [0, 1] èŒƒå›´ã€‚
        actions = (actions - min_act) / (max_act - min_act)
        # å°†å½’ä¸€åŒ–åçš„åŠ¨ä½œæ˜ å°„åˆ°ç¦»æ•£çš„æ¡¶ä¸­ã€‚
        actions *= self.num_bins_actions
        # å››èˆäº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•°ï¼Œå¾—åˆ°ç¦»æ•£çš„æ¡¶ç´¢å¼•ã€‚
        actions = torch.round(actions).long()
        # å°†åŠ¨ä½œå¼ é‡å±•å¹³ï¼Œä¾¿äºåç»­å¤„ç†;ä¿æŒæ‰¹æ¬¡ç»´åº¦ä¸å˜ï¼Œå°†æ‰€æœ‰å…¶ä»–ç»´åº¦å±•å¹³ã€‚
        actions = actions.reshape(actions.shape[0], -1)
        # å°†ç¦»æ•£çš„åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºæ–‡æœ¬å­—ç¬¦ä¸²ï¼›ç”¨ç©ºæ ¼è¿æ¥
        action_txt = [" ".join(map(str, x.tolist())) for x in actions]

        return action_txt

    def get_action_from_text_action(self, action_txt, instruction=None):
        """
        Get the action from the text action.
        :param action_txt: List[str], the text action.
        :param instruction: str, the instruction for the current episode. This is needed for libero bounds 99% as the action space is different for different instructions.
        """
        # TODO: implement this for ee action space
        bs = len(action_txt)
        min_act = torch.tensor(self.dataset_stats["min"])
        max_act = torch.tensor(self.dataset_stats["max"])

        try:
            """ "
            Note: The action_txt is a list of strings. action_txt[i] is the action text for the i-th sample.
            action_txt[i] is a string that contains horizon * act_dim numbers in space separated format.
            We have built in some flexbility for handling minor mistakes in the action_txt.
            """
            # remove space from the front and back of the action_txt if they exist # ç§»é™¤å‰åç©ºæ ¼
            action_txt = [x.strip() for x in action_txt]
            # æŒ‰ç©ºæ ¼åˆ†å‰²å­—ç¬¦ä¸²
            action = [[x for x in _action_txt.split(" ")] for _action_txt in action_txt]
            action = torch.tensor(
                [
                    [int(x) for x in _action_txt if x != ""] for _action_txt in action
                ],  # å¢åŠ äº†if x != "" ä¿®å¤ "" çš„æƒ…å†µ
                dtype=torch.float32,
            )
            # This handles tha case when bs == 1 and the action_txt is not divisible by act_dim
            # We remove some elements so that it is divisible by act_dim
            # å½“å•ä¸ªæ ·æœ¬çš„åŠ¨ä½œæ•°é‡ä¸æ˜¯ act_dim çš„æ•´æ•°å€æ—¶ï¼Œæˆªæ–­å¤šä½™éƒ¨åˆ†ã€‚
            if bs == 1 and len(action[0]) % self.act_dim != 0:
                action = action[0][: len(action[0]) - len(action[0]) % self.act_dim][
                    None, :
                ]

            # reshape to (bs, -1, act_dim)
            # takes care of case when the action_txt has less than horizon * act_dim numbers
            # å°†æ‰å¹³çš„åŠ¨ä½œåºåˆ—é‡å¡‘ä¸º [æ‰¹æ¬¡å¤§å°, æ—¶é—´æ­¥æ•°, åŠ¨ä½œç»´åº¦]ã€‚
            action = action.reshape(bs, -1, self.act_dim)
            # if action.shape[1] < self.horizon, pad the action with the last action
            # ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œå¡«å……åˆ°ç›®æ ‡é•¿åº¦ horizonã€‚
            if action.shape[1] < self.horizon:
                action = torch.cat(
                    [
                        action,
                        action[:, -1:].repeat(1, self.horizon - action.shape[1], 1),
                    ],
                    dim=1,
                )
            # æ—¶é—´æ­¥è¶…å‡ºæ—¶æˆªæ–­
            if action.shape[1] > self.horizon:
                action = action[:, : self.horizon]
            # åç¦»æ•£åŒ–ä¸ºè¿ç»­åŠ¨ä½œ
            action = ((action / self.num_bins_actions) * (max_act - min_act)) + min_act
        except Exception as e:
            print(f"Error in parsing action text: {e}")
            print(action_txt)
            # è§£æå¤±è´¥æ—¶è¿”å›å®‰å…¨çš„é»˜è®¤åŠ¨ä½œï¼ˆåŠ¨ä½œèŒƒå›´çš„ä¸­ç‚¹ï¼‰
            action = ((min_act + max_act) / 2).repeat(bs, self.horizon, 1)

        return action

    def check_inputs(
        self,
        pc,
        rgb_pc,
        instr,
        rgb,
        ori_act,
        ee_pos,
        ee_rot,
        ee_gri,
        out_ee_pos,
        out_ee_rot,
        out_ee_gri,
        out_ori_act,
        get_loss,
        get_action,
        get_one_step_action,
        last_action_txt,
    ):
        """
        è¿™ä¸ª check_inputs æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª â€œå…¥å£é˜²å¾¡å‡½æ•°â€ï¼š
        åœ¨ forward æˆ– __call__ çœŸæ­£å¹²æ´»å‰ï¼ŒæŠŠæ‰€æœ‰è¾“å…¥å’Œé…ç½®å…ˆæ ¡éªŒä¸€éï¼Œé˜²æ­¢å‡ºç°ã€Œé…é”™æ¨¡å¼ + ä¼ é”™æ•°æ®ã€å¯¼è‡´çš„éšè”½ bugã€‚
        """
        # æ¨¡å‹å‡è®¾è‡ªå·±å·²ç»è¢«â€œåˆå§‹åŒ–å®Œæ¯•â€ï¼ˆæœ‰ dataset_statsï¼‰ï¼Œä¸åœ¨ forward é‡Œä¸´æ—¶è¡¥é½ã€‚
        assert (
            self.dataset_stats is not None
        ), "dataset_stats must be set before calling forward"
        assert isinstance(instr, list)

        # æ¨¡å¼äº’æ–¥ï¼šè¦ä¹ˆç®— lossï¼Œè¦ä¹ˆå‡º action
        assert not (get_loss and get_action)
        assert get_loss or get_action

        # ä¸€æ­¥ä¸€æ­¥åŠ¨ä½œæ¨¡å¼çš„é¢å¤–çº¦æŸ
        # ä¸€æ­¥ä¸€æ­¥ç”ŸæˆåŠ¨ä½œ = â€œåœ¨çº¿æ§åˆ¶â€åœºæ™¯ï¼Œå½“å‰å®ç°åªæ”¯æŒå•æ¡è½¨è¿¹ï¼Œä¸æ”¯æŒ batch ç‰ˆæœ¬ã€‚
        if get_one_step_action:
            assert get_action
            assert isinstance(last_action_txt, str)
            assert len(instr) == 1, "one_step_action is only supported for batch size 1"
        # å¯¹ RGB è¾“å…¥çš„ç¡¬æ€§æ£€æŸ¥ï¼ˆå½¢çŠ¶ + æ•°å€¼èŒƒå›´ï¼‰
        if self.rgb_input:
            assert rgb is not None
            # [B, history, num_cam, H, W, 3]
            assert rgb.shape[1:] == (
                self.history,
                self.num_cam,
                *self.rgb_img_size,
                3,
            ), f"rgb.shape: {rgb.shape}, self.history: {self.history}, self.num_cam: {self.num_cam}, self.rgb_img_size: {self.rgb_img_size}"
            # some room for numerical errors
            # 0, 2, 255 are valid values
            assert (rgb.min() >= -1e-2) and (
                1.99 <= rgb.max() <= 255.01
            ), f"rgb.min(): {rgb.min()}, rgb.max(): {rgb.max()}"
        else:
            assert rgb is None

        assert pc is None
        assert rgb_pc is None

    def get_imgs(
        self,
        bs,
        pc,
        rgb_pc,
        rgb,
    ):
        """
        Get the images for the given inputs
        :param bs: int, the batch size
        :param pc: torch.Tensor, the point cloud
        :param rgb_pc: torch.Tensor, the rgb point cloud
        :param rgb: torch.Tensor, the rgb image
        :return: list of list of PIL images
        """
        imgs = [[] for _ in range(bs)]

        if self.rgb_input:
            for i, _rgb in enumerate(rgb):
                _imgs = []
                for j in range(self.history):
                    for k in range(self.num_cam):
                        _imgs.append(_rgb[j][k])
                # _imgs é‡Œæ˜¯æŸä¸ªbatchsizeçš„æ‰€æœ‰å†å²ï¼Œæ‰€æœ‰è§†è§’ï¼Œç„¶åå…¨éƒ¨æ‹¼ä¸€èµ·
                if self.tiled_rgb_imgs:
                    _imgs = [self.tile_images(_imgs)]
                _imgs = [
                    Image.fromarray(x.cpu().numpy().astype(np.uint8)) for x in _imgs
                ]
                imgs[i].extend(_imgs)

        return imgs

    def get_qwen_inputs(
        self,
        bs: int,  # æ‰¹æ¬¡å¤§å°
        imgs: List[List[PIL.Image.Image]],  # å›¾åƒåˆ—è¡¨çš„åˆ—è¡¨ [[img1, img2], [img3, img4], ...]
        instr: List[str],  # æŒ‡ä»¤åˆ—è¡¨ ["æŒ‡ä»¤1", "æŒ‡ä»¤2", ...]
        action_txt: List[str],  # åŠ¨ä½œæ–‡æœ¬åˆ—è¡¨ ["100 200", "150 250", ...]
        drop_assistant: bool = False,  # æ˜¯å¦ä¸¢å¼ƒåŠ©æ‰‹å›å¤éƒ¨åˆ†
        add_generation_prompt: bool = False,  # æ˜¯å¦æ·»åŠ ç”Ÿæˆæç¤º
    ):
        """
        è¿™ä¸ªæ–¹æ³•å°†å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒã€æŒ‡ä»¤ã€åŠ¨ä½œæ–‡æœ¬ï¼‰è½¬æ¢ä¸ºQwenæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æ ‡å‡†åŒ–æ ¼å¼ã€‚
        Get the Qwen inputs for the given inputs
        :param bs: int, the batch size
        :param imgs: list of list of PIL images
        :param instr: list of strings
        :param action_txt: list of strings
        :param drop_assistant: bool, whether to drop the assistant portion.
        :param add_generation_prompt: bool, whether to add the generation prompt assistant\n.
        """
        """
        # æ¯ä¸ªexampleçš„æ ¼å¼ç±»ä¼¼ï¼š
        system: ç³»ç»Ÿè§’è‰²è®¾å®š
        user: ç”¨æˆ·è¾“å…¥ï¼ŒåŒ…å«ï¼š
            å›¾åƒï¼ˆä¸€å¼ æˆ–å¤šå¼ ï¼‰
            æ–‡æœ¬æŒ‡ä»¤
        assistant: åŠ©æ‰‹å›å¤ï¼ˆé€šå¸¸æ˜¯åŠ¨ä½œæŒ‡ä»¤ï¼‰
        """
        examples = [
            format_data(
                system_message=self.system_message,
                image=imgs[i],
                instr=instr[i],
                action_txt=action_txt[i],
            )
            for i in range(bs)
        ]
        if drop_assistant:
            # drop the assistant portion so the model must generate it
            examples = [e[:2] for e in examples]
        # åªç®¡ï¼šâ€œæŠŠä¸€å † {role, content} å˜æˆä¸€æ¡ prompt å­—ç¬¦ä¸²â€
        # ç®¡çš„æ˜¯ã€Œå¯¹è¯æ ¼å¼ã€ï¼Œä¸ç®¡å›¾ç‰‡åƒç´ 
        # æœ€å¤šä¼šæ’ä¸€äº› <image> / <vision_start> è¿™ç§å ä½ç¬¦ï¼ˆæ ‡è®°â€œè¿™é‡Œæœ‰å›¾â€ï¼‰
        texts = [
            self.processor.apply_chat_template(
                example,
                tokenize=False,  # å…¶å®è¿™ encode è¡¨è¿°æ›´åˆé€‚å§ï¼›å› ä¸º tokenizer çš„ tokenize å’Œ encode çš„é€»è¾‘ï¼›VL æ¨¡å‹ï¼ˆå¸¦å›¾åƒï¼‰çš„å¤„ç†é“¾å¿…é¡» tokenize=False â†’ å†è®© processor(text+image) encode
                add_generation_prompt=add_generation_prompt,  # æ§åˆ¶æ˜¯å¦åœ¨æœ€åè‡ªåŠ¨åŠ ä¸Š assistant çš„èµ·å§‹æ¨¡æ¿ã€‚æç¤ºè¦å¼€å§‹ç”Ÿæˆäº†
                add_vision_id=self.add_vision_id,  # Qwen-VL ç‰¹æœ‰ï¼›æ˜¯å¦è¦è‡ªåŠ¨åœ¨ user çš„æ–‡æœ¬é‡Œæ’å…¥ vision token æ ‡è¯†
            )
            # when add_generation_prompt is True, it will add the prompt
            # `assistant\n` to the end of the input text
            for example in examples
        ]
        # [0] in process_vision_info is for image input, [1] is for video input
        # ä»ä¸€æ®µå¯¹è¯ç»“æ„é‡Œï¼ŒæŠŠæ‰€æœ‰è·Ÿè§†è§‰ç›¸å…³çš„ä¿¡æ¯ï¼ˆå›¾ç‰‡ / è§†é¢‘ï¼‰æŠ½å‡ºæ¥ï¼Œæ•´ç†æˆ Qwen-VL èƒ½åƒçš„ images è¾“å…¥æ ¼å¼ã€‚
        """
        process_vision_info
        ğŸ‘‰ åªç®¡ï¼šâ€œä»å¯¹è¯ç»“æ„é‡ŒæŠŠçœŸæ­£çš„å›¾ç‰‡å¯¹è±¡æ‹å‡ºæ¥â€
        æ¯”å¦‚æŸæ¡ message çš„ content é‡Œå‡ºç°
        {"type": "image", "image": pil_img}
        å®ƒæŠŠè¿™äº› PIL / numpy å…¨éƒ¨æ”¶é›†æˆä¸€ä¸ª images åˆ—è¡¨
        """
        image_inputs = [process_vision_info(example)[0] for example in examples]

        """
        ä½ ç°åœ¨ VLA çš„åœºæ™¯ç¡®å®å¾ˆç®€å•ï¼š
        ä½ å·²ç»æ‰‹é‡Œæœ‰ä¸€ä¸ª imgs = [list of PIL.Image]ï¼Œ
        æ„Ÿè§‰ç›´æ¥ä¸¢è¿› processor(images=imgs, text=æŒ‡ä»¤) å°±è¡Œäº†ï¼Œå¯¹å§ï¼Ÿ
        å¦‚æœä½ è‡ªå·±å†™ä¸€å¥— VLMï¼Œæ˜¯å¯ä»¥è¿™ä¹ˆå¹²çš„ã€‚
        ä½†è¿™é‡Œå®ƒè¦å…¼å®¹çš„æ˜¯ï¼š
        ä»»æ„å¯¹è¯ç»“æ„ï¼ˆå¤šè½® system/user/assistantï¼‰
        ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡ï¼ˆå¯èƒ½åœ¨ user çš„ç¬¬ 2 å¥è¯ä¸­é—´æ‰æ’ä¸€å¼ å›¾ï¼‰
        ç”šè‡³ä¸€ä¸ª message é‡Œ textã€image äº¤é”™å‡ºç°
        """
        model_inputs = self.processor(
            text=texts, images=image_inputs, return_tensors="pt", padding=True
        )

        # ä¸€é”®æŠŠæ‰€æœ‰è¾“å…¥æ¬åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ä¸Šçš„å°å¥—è·¯ï¼Œæ²¡æœ‰åˆ«çš„é»‘é­”æ³•ã€‚
        for key in model_inputs:
            model_inputs[key] = model_inputs[key].to(
                next(self.model.parameters()).device
            )
        return model_inputs, examples

    def forward(
        self,
        pc=None,
        rgb_pc=None,
        instr=None,
        rgb=None,
        ori_act=None,
        ee_pos=None,
        ee_rot=None,
        ee_gri=None,
        out_ee_pos=None,
        out_ee_rot=None,
        out_ee_gri=None,
        out_ori_act=None,
        get_loss=True,
        get_action=False,
        generate_temperature=0.1,
        get_one_step_action=False,
        last_action_txt="",
    ):
        """
        Forward pass for the Qwen model

        Parameters
        ----------
        pc : optional
            Point cloud data
        rgb_pc : optional
            RGB point cloud data
        instr : optional
            Instructions
        rgb : optional
            RGB images
        ori_act : optional
            Original actions è¾“å…¥æ¡ä»¶ï¼ˆå†å²ï¼‰
        ee_pos : optional
            End effector position
        ee_rot : optional
            End effector rotation
        ee_gri : optional
            End effector gripper state
        out_ee_pos : optional
            Output end effector position è®­ç»ƒæ ‡ç­¾ï¼ˆæœªæ¥ï¼‰
        out_ee_rot : optional
            Output end effector rotation
        out_ee_gri : optional
            Output end effector gripper state
        out_ori_act : optional
            Output original actions
        get_loss : bool, default=True
            Whether to calculate and return loss
        get_action : bool, default=False
            Whether to get actions
        generate_temperature : float, default=0.1
            Temperature for generation
        get_one_step_action : bool, default=False
            Whether to run the model forward for only one step of action at a time. If True,
            we complete the last_action_txt to next action string sufficient to decode the
            action for one next step. If False, we complete the last_action_txt to next
            action string sufficient to decode the action for all the next steps.
        last_action_txt : str, default=""
            The last action text to complete to get the next action text. This is only
            used when get_one_step_action is True.

        Returns
        -------
        dict or tuple
            Model outputs (should specify exact return structure)

        Raises
        ------
        Exception
            Any exceptions that can be raised (if applicable)
        """
        self.check_inputs(
            pc=pc,
            rgb_pc=rgb_pc,
            instr=instr,
            rgb=rgb,
            ori_act=ori_act,
            ee_pos=ee_pos,
            ee_rot=ee_rot,
            ee_gri=ee_gri,
            out_ee_pos=out_ee_pos,
            out_ee_rot=out_ee_rot,
            out_ee_gri=out_ee_gri,
            out_ori_act=out_ori_act,
            get_loss=get_loss,
            get_action=get_action,
            get_one_step_action=get_one_step_action,
            last_action_txt=last_action_txt,
        )
        # æ”¯æŒ batch è®­ç»ƒ/æ¨ç†
        bs = len(instr)

        # imgs is list of list of PIL images
        # # _imgs é‡Œæ˜¯æŸä¸ªbatchsizeçš„æ‰€æœ‰å†å²ï¼Œæ‰€æœ‰è§†è§’ï¼Œç„¶åå…¨éƒ¨æ‹¼ä¸€èµ·ï¼›ç¡¬æ‹¼ï¼›åªæ”¯æŒrgb
        imgs = self.get_imgs(
            bs=bs,
            pc=pc,
            rgb_pc=rgb_pc,
            rgb=rgb,
        )

        # TODO: implement this for ee action space
        # ä½ è¿™æ¬¡è°ƒç”¨ forward æ²¡ä¼ â€œç›®æ ‡åŠ¨ä½œåºåˆ—â€è¿›æ¥ã€‚å…¸å‹åœºæ™¯ï¼šçº¯æ¨ç†ï¼›ç»™åé¢ç•™ä¸€ä¸ªå ä½çš„ action_txtï¼Œå½¢çŠ¶æ˜¯â€œbatch å¤§å°çš„ç©ºåˆ—è¡¨â€
        if out_ori_act is None:
            assert not get_loss
            action_txt = [[]] * bs
        else:
            # ä½ æŠŠ ground-truth åŠ¨ä½œåºåˆ—ï¼ˆåŸå§‹åŠ¨ä½œè½¨è¿¹ï¼‰ä¼ è¿›æ¥äº†ã€‚å…¸å‹åœºæ™¯ï¼šè®­ç»ƒ / ç›‘ç£æ¨¡å¼
            action_txt = self.get_text_action(out_ori_act, instruction=instr)

        model_inputs, examples = self.get_qwen_inputs(
            bs=bs,
            imgs=imgs,
            instr=instr,
            action_txt=action_txt,
            drop_assistant=get_action,  # when getting action, we drop the assistant portion
            add_generation_prompt=get_action,  # when getting action, we add the generation prompt assistant\n so that the model need not generate it
        )

        """
        æŠŠä¸è¯¥ç®— loss çš„ token éƒ½æ¢æˆ -100ï¼ˆCrossEntropy çš„ ignore_indexï¼‰
        é¡ºä¾¿å¯¹ä¸€éƒ¨åˆ†åŠ¨ä½œ token åšâ€œé—®å·é®æŒ¡å¢å¼ºâ€
        """
        if get_loss:
            labels = model_inputs["input_ids"].clone()  # å®Œæ•´è¾“å…¥åºåˆ—
            # mask system message and image token IDs in the labels
            for i, example in enumerate(examples):
                if (self._sysuser_len is None) or (not self.cache_sysuser_len):
                    sysuser_conv = example[:-1]  # åªä¿ç•™ [system, user]ï¼Œå»æ‰æœ€åçš„åŠ¨ä½œé‚£æ¡ message
                    sysuser_text = self.processor.apply_chat_template(
                        sysuser_conv, tokenize=False, add_vision_id=self.add_vision_id
                    )
                    sysuser_img, _ = process_vision_info(sysuser_conv)

                    # è¿”å›å…³é”®é”® input_ids æ–‡æœ¬token IDs (batch_size, sequence_length)ï¼›attention_mask - æ³¨æ„åŠ›æ©ç ï¼›pixel_values - å›¾åƒç‰¹å¾ç­‰
                    sysuser_inputs = self.processor(
                        text=[sysuser_text],
                        images=[sysuser_img],
                        return_tensors="pt",
                        padding=True,  # æ¯ä¸ªbsçš„è¡¥å……åˆ°ä¸€æ ·é•¿
                    )

                    sysuser_len = sysuser_inputs["input_ids"].shape[1]
                    sysuser_len += 3  # to mask out `assistant\n`
                    # â€œå‰ç¼€é•¿åº¦â€ï¼šä»åºåˆ—å¼€å¤´ï¼Œä¸€ç›´åˆ°åŠ¨ä½œæ•°å­—ä¸²å¼€å§‹ä¹‹å‰çš„ token æ€»é•¿åº¦ã€‚
                    self._sysuser_len = sysuser_len
                else:
                    sysuser_len = self._sysuser_len
                # TIP: to decode the input use:ä¸åŒå¡«å……æ–¹å‘çš„è§£ç æ–¹æ³•;å®¹æ˜“ç†è§£
                # when padding is right: self.processor.decode(model_inputs["input_ids"][0][0:sysuser_len])
                # when padding is left: self.processor.decode(model_inputs["input_ids"][0][num_pad_tokens: num_pad_tokens + sysuser_len])
                """
                å‘Šè¯‰ CrossEntropyï¼š
                â€œè¿™äº› token æ˜¯ system+user+vision çš„ promptï¼Œä¸æ˜¯è¦é¢„æµ‹çš„ç›®æ ‡ï¼Œåˆ«ç®— lossã€‚â€
                """

                if self.processor.tokenizer.padding_side == "right":
                    labels[i, :sysuser_len] = -100
                elif (
                    self.processor.tokenizer.padding_side == "left"
                ):  # ï¼ˆç†è®ºæ”¯æŒï¼Œå®é™…ä¸Šç”¨ assert ç¦äº†ï¼‰ä¸å½±å“FlashAttention
                    """
                    ç–‘ä¼¼æœ‰bug.
                    pad_token_id = self.processor.tokenizer.pad_token_id
                    num_pad_tokens = sum(labels[i] == pad_token_id).item()
                    labels[i, num_pad_tokens : num_pad_tokens + sysuser_len] = -100
                    """
                    num_pad_tokens = sum(labels[i] == 151643).item()
                    labels[i, num_pad_tokens : num_pad_tokens + sysuser_len] = -100
                else:
                    raise ValueError(
                        f"Unknown padding side: {self.processor.tokenizer.padding_side}"
                    )

                assert (
                    not self.processor.tokenizer.padding_side
                    == "left"  # ï¼ˆç†è®ºæ”¯æŒï¼Œå®é™…ä¸Šç”¨ assert ç¦äº†ï¼‰
                ), "current implementation only supports right padding"

                """
                â€œæŠŠ token id åºåˆ—ç¿»è¯‘å›äººè¯â€ï¼Œ
                æ–¹ä¾¿ä½ æ£€æŸ¥ï¼š
                â€“ è¾“å…¥åºåˆ—é•¿å•¥æ ·
                â€“ mask / padding / sysuser_len æœ‰æ²¡æœ‰å¼„é”™ã€‚
                -100 ä¸æ˜¯åˆæ³•çš„ token idï¼Œprocessor.decode() æ ¹æœ¬ decode ä¸å‡ºæ¥ï¼Œç”šè‡³ç›´æ¥æŠ¥é”™æˆ–è€…ç»™ä½ å¥‡æ€ªä¸œè¥¿ã€‚
                è¿™ä¸¤è¡Œ TIP æ˜¯ä¸ºäº† debug è¾“å…¥ï¼Œè€Œä¸æ˜¯ debug labelsï¼Œæ‰€ä»¥ç”¨çš„æ˜¯ model_inputsã€‚
                """
                # for debugging, compare ï¼› decode-ã€‹æŠŠä¸€ä¸² token id â†’ è¿˜åŸæˆå¯è¯»æ–‡æœ¬å­—ç¬¦ä¸²ã€‚
                # ä½¿ç”¨æ³¨æ„åŠ›æ©ç è¿‡æ»¤åçš„è§£ç 
                # self.processor.decode(model_inputs["input_ids"][i][model_inputs["attention_mask"][i] == 1])
                # ä¸åŸå§‹åºåˆ—çš„è§£ç å¯¹æ¯”
                # with self.processor.decode(model_inputs["input_ids"][i])
                _action_txt = action_txt[i]
                # 10% of sample has no augmentation
                if random.random() < 0.1:
                    _action_mask_aug_per = 0.0
                else:
                    _action_mask_aug_per = random.uniform(0.0, self.action_mask_aug_per)
                mask_len = int(len(_action_txt) * _action_mask_aug_per)
                mask_indices = random.sample(range(len(_action_txt)), mask_len)
                mask_indices = [
                    x + sysuser_len for x in mask_indices
                ]  # add sysuser_len to the mask indices to get the correct indices of these tokens
                labels[
                    i, mask_indices
                ] = -100  # these elements will not be used for loss calculation
                model_inputs["input_ids"][
                    i, mask_indices
                ] = 30  # replace the input ids with '?' token id

            labels[labels == 151643] = -100

            outputs = self.model(**model_inputs)
            logits = outputs.logits  # (batch_size, seq_len, vocab_size)

            # copied from modeling_qwen2_5_vl.py to compute the loss
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            """
            æœ‰äº›æ¨¡å‹åœ¨å‰å‘é‡Œç”¨çš„æ˜¯ half precisionï¼ˆæ¯”å¦‚ float16 / bfloat16ï¼‰ï¼Œ
            ä¸ºäº†ç®— loss æ›´ç¨³å®šï¼Œè¿™é‡Œç»Ÿä¸€è½¬æˆ float32ï¼š
            é¿å…æ•°å€¼ä¸ç¨³å®šï¼ˆç‰¹åˆ«æ˜¯ softmax + log çš„æ—¶å€™å®¹æ˜“æº¢å‡º/ä¸‹æº¢ï¼‰
            å®Œå…¨ä¸æ”¹å½¢çŠ¶ï¼Œåªæ˜¯æ”¹ dtype
            """
            logits = logits.float()
            # Shift so that tokens < n predict n
            """
            # å‡è®¾åºåˆ—é•¿åº¦=6
            è¾“å…¥åºåˆ—: [S, I, U, START, A1, A2]
            Labels:   [-100, -100, -100, -100, 100, 200]

            # Shiftæ“ä½œï¼š
            shift_logits = é¢„æµ‹[S,I,U,START,A1]  # ä½ç½®0-4çš„é¢„æµ‹
            shift_labels =        [-100, -100, -100, 100, 200]  # ä½ç½®1-5çš„çœŸå®å€¼

            # è®¡ç®—æŸå¤±æ—¶ï¼š
            é¢„æµ‹[I] vs çœŸå®[-100] â†’ å¿½ç•¥
            é¢„æµ‹[U] vs çœŸå®[-100] â†’ å¿½ç•¥
            é¢„æµ‹[START] vs çœŸå®[-100] â†’ å¿½ç•¥
            é¢„æµ‹[A1] vs çœŸå®[100] â†’ è®¡ç®—æŸå¤± âœ“
            é¢„æµ‹[A2] vs çœŸå®[200] â†’ è®¡ç®—æŸå¤± âœ“
            """
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            """
            # shift_logits å½¢çŠ¶: (batch_size, seq_len-1, vocab_size)
            # ä¾‹å¦‚: (2, 5, 51200) - 2ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬5ä¸ªä½ç½®ï¼Œè¯æ±‡è¡¨å¤§å°51200
            # shift_labels å½¢çŠ¶: (batch_size, seq_len-1)
            # ä¾‹å¦‚: (2, 5) - 2ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬5ä¸ªæ ‡ç­¾
            # shift_logits å½¢çŠ¶: (batch_size * (seq_len-1), vocab_size)
            # ä¾‹å¦‚: (10, 51200) - æ€»å…±10ä¸ªä½ç½®ï¼Œæ¯ä¸ªä½ç½®å¯¹åº”51200ä¸ªè¯æ±‡çš„é¢„æµ‹åˆ†å¸ƒ

            # shift_labels å½¢çŠ¶: (batch_size * (seq_len-1),)
            # ä¾‹å¦‚: (10,) - æ€»å…±10ä¸ªä½ç½®çš„æ­£ç¡®æ ‡ç­¾
            """
            shift_logits = shift_logits.view(-1, shift_logits.size(-1))
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            """
            # CrossEntropyLoss æœŸæœ›çš„è¾“å…¥æ ¼å¼ï¼š
            # input: (N, C) - Nä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬Cä¸ªç±»åˆ«çš„é¢„æµ‹åˆ†å¸ƒ
            # target: (N,) - Nä¸ªæ ·æœ¬çš„æ­£ç¡®ç±»åˆ«ç´¢å¼•
            """
            # self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100) å·²å†™è¿‡
            loss = self.loss_fn(shift_logits, shift_labels)

            return {"loss": loss}

        if get_action:
            sample_args = {}
            if generate_temperature > 0:
                sample_args["temperature"] = generate_temperature
            else:
                sample_args[
                    "do_sample"
                ] = False  # greedy search, this makes the generation deterministic

            if get_one_step_action:
                # we calculate the max number of tokens to generate for one step of action
                # +1 is for the space token
                """
                è¿™é‡Œè¿›å…¥ã€Œåªç”Ÿæˆä¸€æ­¥åŠ¨ä½œã€çš„æ¨¡å¼ã€‚
                self.act_dimï¼šåŠ¨ä½œç»´åº¦æ•°ï¼Œä¾‹å¦‚ 4ã€‚
                len(str(self.num_bins_actions))ï¼š
                    å¦‚æœ num_bins_actions = 1000
                    str(1000) = "1000"ï¼Œé•¿åº¦æ˜¯ 4
                    ç”¨é•¿åº¦ 4 æ¥è¿‘ä¼¼ã€Œæ¯ä¸ªæ•°å­—æœ€å¤šéœ€è¦ 4 ä¸ªå­—ç¬¦ã€ã€‚
                +1ï¼šå¤šç•™ä¸€ä¸ªä½ç½®ç»™ ç©ºæ ¼ï¼Œå› ä¸ºæ–‡æœ¬åŠ¨ä½œéƒ½æ˜¯ "123 456 ...".
                max_new_tokens = 4 * (4 + 1) = 20
                ä¹Ÿå°±æ˜¯ï¼šæœ€å¤šç»™ 20 ä¸ª token æ¥å†™å®Œè¿™ä¸€å¸§åŠ¨ä½œï¼ˆ4 ç»´ Ã— æ¯ç»´æœ€å¤š 4 ä½æ•° + ç©ºæ ¼ï¼‰

                """
                max_new_tokens = self.act_dim * (len(str(self.num_bins_actions)) + 1)
                if last_action_txt != "":
                    # å¦‚æœä¹‹å‰å·²ç»æœ‰ last_action_txtï¼ŒæŠŠå®ƒæ¥åœ¨ prompt åé¢
                    # æŠŠ "512 230 010 999 " è¿™ç§å­—ç¬¦ä¸² tokenization æˆ idsï¼Œå½¢çŠ¶å¤§æ¦‚æ˜¯ [1, T_last]ã€‚
                    last_action_txt_ids = self.processor.tokenizer(
                        last_action_txt, return_tensors="pt"
                    )["input_ids"].to(model_inputs["input_ids"].device)
                    """
                    æŠŠè¿™æ®µå†å²åŠ¨ä½œæ–‡æœ¬ æ‹¼åˆ°å½“å‰è¾“å…¥çš„æœ«å°¾ï¼š
                    model_inputs["input_ids"] åŸæ¥æ˜¯ [1, T_prompt]ï¼Œ
                    æ‹¼å®Œå˜æˆ [1, T_prompt + T_last]ã€‚
                    â†’ ç›¸å½“äºå‘Šè¯‰æ¨¡å‹ï¼šã€Œå‰é¢å·²ç»ç”Ÿæˆäº†è¿™ä¹ˆå¤šåŠ¨ä½œäº†ï¼Œæ¥ç€å†™ä¸‹ä¸€æ­¥ã€ã€‚
                    """
                    model_inputs["input_ids"] = torch.cat(
                        [model_inputs["input_ids"], last_action_txt_ids], dim=1
                    )
                    model_inputs["attention_mask"] = torch.cat(
                        [
                            model_inputs["attention_mask"],
                            torch.ones_like(last_action_txt_ids),
                        ],
                        dim=1,
                    )

            # token id to text mapping
            # 220 is space
            # 15 to 24 are 0 to 9
            # 151645 is the end of text token
            generated_ids = self.model.generate(
                **model_inputs,
                logits_processor=[self.logits_processor],
                max_new_tokens=(
                    max_new_tokens
                    if get_one_step_action
                    else self.num_tokens  # å¦‚æœæ˜¯ get_one_step_action=Trueï¼Œå°±æ˜¯åˆšæ‰ç®—å‡ºçš„ max_new_tokens=20ï¼›å¦åˆ™ç”¨ self.num_tokensï¼Œä»£è¡¨ä¸€æ•´æ®µåŠ¨ä½œçš„é•¿åº¦ä¸Šé™ã€‚
                ),
                **sample_args,
            )

            input_ids = model_inputs["input_ids"]
            # æŠŠæ–°ç”Ÿæˆçš„éƒ¨åˆ†ä»åŸè¾“å…¥é‡Œåˆ‡å‡ºæ¥
            generated_ids_trimmed = [
                out_ids[len(in_ids) :]
                for in_ids, out_ids in zip(input_ids, generated_ids)
            ]
            # æŠŠ token id è½¬å›å­—ç¬¦ä¸²åŠ¨ä½œ
            generated_action_txt = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )

            if get_one_step_action:
                # TODO: Only supports batch size 1
                # ä¸€æ­¥ä¸€æ­¥ç”Ÿæˆæ¨¡å¼ä¸‹ï¼Œè¦æŠŠæ–°ä¸€æ­¥æ¥å›å†å²
                """
                last_action_txt = "512 230 010 999 "
                generated_action_txt[0] = "123 456 789 001 "ï¼ˆåˆšç”Ÿæˆçš„ä¸€å¸§ï¼‰
                åˆå¹¶åï¼š
                generated_action_txt = ["512 230 010 999 123 456 789 001 "]
                ä¸‹æ¬¡å†è°ƒç”¨ get_one_step_action=True æ—¶ï¼Œä¼šæŠŠè¿™ä¸ªå®Œæ•´ä¸²åˆæ¥å›å»ç»§ç»­å†™ä¸‹ä¸€æ­¥ã€‚
                """
                generated_action_txt = [last_action_txt + generated_action_txt[0]]

            # æŠŠæ–‡æœ¬åŠ¨ä½œè§£æå›æ•°å€¼åŠ¨ä½œ
            out_ori_act = self.get_action_from_text_action(
                generated_action_txt, instruction=instr
            )
            """
            out_ori_act åŸ shapeï¼š[B, T, act_dim]ï¼ˆæ¯”å¦‚ [1, 5, 4]ï¼‰
            out_ori_act[:, -1:]ï¼šåªä¿ç•™æœ€åä¸€å¸§ï¼ˆæœ€æ–°ç”Ÿæˆçš„ä¸€å¸§ï¼‰ï¼Œå½¢çŠ¶ [1, 1, 4]ã€‚
            è¿™æ ·ä¸€æ­¥æ­¥æ§åˆ¶æ—¶ï¼Œä¸Šå±‚æ§åˆ¶å¾ªç¯æ‹¿åˆ°çš„å°±æ˜¯ã€Œå½“å‰è¿™ä¸€å¸§åŠ¨ä½œã€ã€‚
            """
            if get_one_step_action:
                out_ori_act = out_ori_act[:, -1:]

            return {
                "gt_action_text": action_txt,
                "pred_action_txt": generated_action_txt,
                "gt_out_ori_act": out_ori_act,
                "out_ori_act": out_ori_act,
            }

    def save_pretrained(self, path):
        """
        PreTrainedModel.save_pretrained(path) ä¼šåœ¨ path ç›®å½•ä¸‹å†™ä¸€å †æ–‡ä»¶ï¼Œå…¸å‹åŒ…æ‹¬ï¼š
            config.json
                æ¨¡å‹ç»“æ„é…ç½®ï¼ˆå±‚æ•°ã€hidden_sizeã€n_headsã€vocab_sizeã€dropoutã€æ˜¯å¦ç”¨ flash-attn ç­‰ï¼‰
            model.safetensors æˆ– pytorch_model.bin
                æ¨¡å‹æ‰€æœ‰å‚æ•°ï¼ˆæƒé‡ã€åç½®ï¼‰ï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒå‡ºæ¥çš„ checkpoint çœŸæ­£çš„æ ¸å¿ƒ
            å¯èƒ½è¿˜æœ‰ä¸€äº›ï¼š
                generation_config.jsonï¼ˆé»˜è®¤ generate() çš„å‚æ•°ï¼‰
                é¢å¤–çš„è‡ªå®šä¹‰ config å­—æ®µï¼ˆæ¯”å¦‚åŠ äº† LoRAã€action head çš„ä¿¡æ¯ï¼‰

        Processor
        tokenizer.json
        tokenizer_config.json
        special_tokens_map.json
        preprocessor_config.json / image_processor_config.json ç­‰
        """
        self.model.save_pretrained(path)
        self.processor.save_pretrained(path)

    def from_pretrained(self, path, is_trainable=True):
        # å¸¸è§çš„æŠ€å·§ è‡ªåŠ¨è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        # self.parameters() è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å¯å­¦ä¹ å‚æ•°
        # next(self.parameters()) è·å–ç¬¬ä¸€ä¸ªå‚æ•°å¼ é‡ å¯èƒ½æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„æƒé‡æˆ–ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡
        _device = next(self.parameters()).device

        # é˜²æ­¢åœ¨åŠ è½½æ–°æ¨¡å‹æ—¶å‡ºç°å†…å­˜ä¸è¶³çš„é—®é¢˜
        del self.model
        # æ¸…ç©ºGPUç¼“å­˜ï¼Œé¿å…å†…å­˜ç¢ç‰‡
        torch.cuda.empty_cache()
        # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œå½»åº•é‡Šæ”¾å†…å­˜
        gc.collect()

        # LoRAæ¨¡å‹åŠ è½½è·¯å¾„
        if self.use_lora:
            # PEFT (Parameter-Efficient Fine-Tuning) åº“
            # å‚æ•°é«˜æ•ˆå¾®è°ƒçš„ Hugging Face åº“ï¼Œä¸»è¦ç›®çš„æ˜¯ç”¨æå°‘çš„å‚æ•°é‡æ¥å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹
            from peft import PeftModel

            base_model = self.load_qwen_model(
                qwen_model_id=self.qwen_model_id,
                use_lora=self.use_lora,
                use_qlora=self.use_qlora,
                lora_config="",  # None regarless of lora config as lora is added later using PeftModel
                lora_rank=self.lora_rank,  # Doesn't matter here as lora_config is None
                use_flash_attention_2=self.use_flash_attention_2,
                attention_dropout=self.attention_dropout,
            )

            self.model = PeftModel.from_pretrained(
                base_model,
                path,
                is_trainable=is_trainable,
            )
            print("Loading Qwen2.5 PEFT model from", path)
        else:
            extra_kwargs = {}
            if self.use_flash_attention_2:
                extra_kwargs["attn_implementation"] = "flash_attention_2"
            if self.attention_dropout > 0.0:
                config = AutoConfig.from_pretrained(path)
                config.attention_dropout = self.attention_dropout
                extra_kwargs["config"] = config
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                path,
                # device_map={"": "cuda:0"},
                torch_dtype=torch.bfloat16,
                **extra_kwargs,
            )
            print("Loading Qwen2.5 full model from", path)

        if self.use_flash_attention_2:
            self.processor = Qwen2_5_VLProcessor.from_pretrained(
                path,
                min_pixels=self.min_pixel,
                max_pixels=self.max_pixel,
                padding_side="left",
            )
        else:
            self.processor = Qwen2_5_VLProcessor.from_pretrained(
                path,
                min_pixels=self.min_pixel,
                max_pixels=self.max_pixel,
            )

        print("Loading Qwen2.5 processor from", path)

        QwenActor.to(self, _device)

    def to(self, device):
        # è®¾å¤‡è¿ç§»æ–¹æ³•çš„å¢å¼ºå®ç°
        super().to(device)
        # if device is interger like 0 or "0", convert to cuda:0 # å¦‚æœdeviceæ˜¯æ•´æ•°å¦‚0æˆ–"0"ï¼Œè½¬æ¢ä¸ºcuda:0
        if isinstance(device, int) or (isinstance(device, str) and device.isnumeric()):
            device = f"cuda:{device}"
        if hasattr(self, "renderer"):
            # self.renderer = PyTorch3DRenderer()  # 3Dæ¸²æŸ“å¼•æ“
            # self.cameras = PerspectiveCameras() # è™šæ‹Ÿç›¸æœºé›†åˆ
            self.renderer.renderer.device = device  # è®¾ç½®æ¸²æŸ“å™¨è®¾å¤‡
            self.renderer.cameras.to(device)  # è¿ç§»ç›¸æœºå‚æ•°

    def tile_images(self, images):
        """
        Tile images into a single image æŠŠå¤šå¼ å›¾æ¨ªç€æ‹¼æˆä¸€å¼ é•¿å›¾
        :param images: list[Tensor], æ¯ä¸ª Tensor: (H, W, 3)
                    æˆ– 4D Tensor: (bs, H, W, 3)
        :return: Tensor of shape (max_H, sum_W, 3)
        """
        # å¦‚æœæ˜¯ batch tensorï¼Œæ‹†æˆ list
        if isinstance(images, torch.Tensor) and images.dim() == 4:
            images = list(images)

        for img in images:
            assert len(img.shape) == 3, f"img.shape: {img.shape}"
            assert img.shape[2] == 3, f"img.shape: {img.shape}"

        # æ”¶é›†æ‰€æœ‰å›¾åƒçš„ (H_i, W_i)
        heights, widths = zip(*(im.shape[:-1] for im in images))
        total_width = sum(widths)  # æ¨ªå‘æ€»å®½åº¦ = å„å›¾å®½åº¦ç›¸åŠ 
        max_height = max(heights)  # é«˜åº¦ = æ‰€æœ‰å›¾ä¸­æœ€é«˜çš„é‚£ä¸€å¼ 

        # å…ˆå¼€ä¸€å¼ è¶³å¤Ÿå¤§çš„é»‘åº•å›¾ï¼ŒæŠŠæ¯ä¸ªå­å›¾ä¾æ¬¡è´´ä¸Šå»ã€‚
        device = images[0].device
        dst = torch.zeros((max_height, total_width, 3), device=device)

        current_x = 0
        for i, img in enumerate(images):
            h, w, _ = img.shape
            dst[:h, current_x : current_x + w, :] = img
            current_x += w

        return dst
