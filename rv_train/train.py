# Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the CC BY-NC 4.0 license [see LICENSE for details].

import argparse
import gc
import os
import pickle as pkl
import pprint
import random
import shutil
from contextlib import redirect_stdout
from datetime import datetime
from time import time

import roboverse
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
# import tqdm
from torch import autocast
# å¢åŠ äº†FSDPçš„é€»è¾‘
from torch.distributed.fsdp import BackwardPrefetch, FullStateDictConfig
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import (MixedPrecision, ShardingStrategy,
                                    StateDictType)
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

from rv_train import models
from rv_train.configs import get_cfg_defaults
from rv_train.utils import train_utils as utils

DEVICE = ""

START_TIME = time()


def save_checkpoint(name, epoch, model, optimizer, lr_sched, cfg, log_dir):
    """
    Saves all information required for resuming training in the experiment
    folder.
    """
    # take care of DDP
    if isinstance(model, DDP):
        model_module = model.module
    else:
        model_module = model

    # take care of model saving for models that have save_pretrained method
    if hasattr(model_module, "save_pretrained"):
        model_state = None
        print("WARNING: model has save_pretrained method, not saving model state")
        model_module.save_pretrained(f"{log_dir}/model_{name}")
    else:
        model_state = model_module.state_dict()

    # Prepare checkpoint data
    # å¦‚æœæ˜¯ HF æ¨¡å‹ï¼šè¿™é‡Œæ˜¯ Noneï¼Œå› ä¸ºä½ å·²ç»ç”¨ save_pretrained å­˜åˆ°ç›®å½•é‡Œäº†ï¼›å¦‚æœæ˜¯æ™®é€šæ¨¡å‹ï¼šè¿™é‡Œæ˜¯ state_dict().
    checkpoint_data = {
        "cfg": vars(cfg),  # vars(cfg) ä¼šæŠŠå®ƒè½¬æˆæ™®é€šçš„ dictï¼ˆé”®å€¼ä¸ºæ‰€æœ‰å­—æ®µï¼‰ã€‚
        "epoch": epoch,
        "model_state": model_state,
        "optimizer_state": optimizer.state_dict(),
        "lr_sched_state": lr_sched.state_dict() if lr_sched is not None else None,
    }

    pth_path = f"{log_dir}/model_{name}.pth"
    torch.save(checkpoint_data, pth_path)

    # save the dataset stats
    """
    è¿™æ®µæ˜¯è·Ÿä½ å½“å‰å·¥ç¨‹ï¼ˆæ¯”å¦‚ VLA-0ï¼‰å¼ºç›¸å…³çš„é€»è¾‘ï¼š
        cfg.EXP.MODEL æŒ‡æ˜æœ¬æ¬¡å®éªŒçš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ï¼š
            "qwen"ï¼šçº¯ Qwen æ¨¡å‹ï¼Ÿ
            "dp"ï¼šdiffusion policyï¼Ÿ
            "qwen_dp"ï¼šä¸¤è€…æ··åˆï¼Ÿ
        å¯¹è¿™å‡ ç§æ¨¡å‹ï¼Œä»£ç çº¦å®šï¼šæ¨¡å‹å†…éƒ¨æœ‰ä¸€ä¸ªå±æ€§ï¼š
        model_module.original_dataset_stats
        å…¸å‹ç”¨é€”ï¼šè®­ç»ƒå‰æ ¹æ®æ•°æ®é›†ç»Ÿè®¡å¾—åˆ°çš„ä¸€äº›å½’ä¸€åŒ–ä¿¡æ¯ï¼š
            å‡å€¼ / æ–¹å·®ï¼›
            action / state èŒƒå›´ï¼›
            å…¶ä»–ç»Ÿè®¡é‡ï¼ˆä¾‹å¦‚ç”¨äº rescale è¾“å‡ºï¼‰ã€‚
        è¿™é‡Œç”¨ pickle å•ç‹¬å­˜ä¸ºï¼š
        {log_dir}/dataset_stats.pkl
        ä¾¿äºï¼š
            ä»¥åå•ç‹¬åŠ è½½æ¨ç†ç”¨ï¼ˆæ¯”å¦‚åªæœ‰ä¸€ä¸ª ckpt æ²¡æœ‰å…¨é¡¹ç›®ä»£ç æ—¶ï¼Œä¹Ÿèƒ½æ‹¿åˆ° statsï¼‰ï¼›
            æˆ–è€… eval è„šæœ¬ç›´æ¥ç”¨ log_dir/dataset_stats.pkl åšå½’ä¸€åŒ–ã€‚
    ä¹Ÿå°±æ˜¯è¯´ï¼Œæ­¤å¤„é™¤äº† .pthï¼Œåˆé¢å¤–ä¿å­˜äº†ä¸€ä¸ªæ•°æ®é›†ç»Ÿè®¡æ–‡ä»¶ã€‚
    """
    if cfg.EXP.MODEL in ["qwen", "dp", "qwen_dp"]:
        with open(f"{log_dir}/dataset_stats.pkl", "wb") as f:
            pkl.dump(model_module.original_dataset_stats, f)

    print(f"Checkpoint saved to {pth_path}.")


# def load_model(model, model_path, cfg):
#     """
#     Loads a pretrained model from a given path.
#     :param model: model to load
#     :param model_path: path to the pretrained model
#     :param cfg: config object
#     """
#     print(f"Recovering model and checkpoint from {model_path}")
#     checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)

#     # take care of DDP
#     if isinstance(model, DDP):
#         model_module = model.module
#     else:
#         model_module = model

#     # take care of model loading for models that have load_pretrained method
#     if hasattr(model_module, "from_pretrained"):
#         print("WARNING: model has from_pretrained method")
#         assert model_path[-4:] == ".pth"
#         print(f"Loading from {model_path[:-4]}")
#         model_module.from_pretrained(model_path[:-4])
#     else:
#         model_module.load_state_dict(checkpoint["model_state"])

#     # load the dataset stats
#     if cfg.EXP.MODEL in ["qwen", "dp", "qwen_dp"]:
#         log_dir = "/".join(model_path.split("/")[:-1])
#         with open(f"{log_dir}/dataset_stats.pkl", "rb") as f:
#             original_dataset_stats = pkl.load(f)
#             model_module.set_dataset_stats(original_dataset_stats)


#     return model, checkpoint
def load_model(model, model_path, cfg):
    """
    Loads a pretrained model from a given path.
    :param model: model to load
    :param model_path: path to the pretrained model
    :param cfg: config object
    """
    print(f"Recovering model and checkpoint from {model_path}")
    checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)

    # å…¼å®¹ DDP / FSDP
    if isinstance(model, (DDP, FSDP)):
        model_module = model.module
    else:
        model_module = model

    # å¸¦ from_pretrained çš„ç‰¹æ®Šæ¨¡å‹
    if hasattr(model_module, "from_pretrained"):
        print("WARNING: model has from_pretrained method")
        assert model_path[-4:] == ".pth"
        print(f"Loading from {model_path[:-4]}")
        model_module.from_pretrained(model_path[:-4])
    else:
        state = checkpoint["model_state"]
        if isinstance(model, FSDP):
            # FSDP: ç”¨ FULL_STATE_DICT åŠ è½½ï¼Œå†…éƒ¨è‡ªåŠ¨åˆ‡ç‰‡
            full_cfg = FullStateDictConfig(rank0_only=False, offload_to_cpu=True)
            with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_cfg):
                model.load_state_dict(state)
        else:
            model_module.load_state_dict(state)

    # load the dataset statsï¼ˆlog_dir æ¨æ–­é€»è¾‘ä¸å˜ï¼‰
    if cfg.EXP.MODEL in ["qwen", "dp", "qwen_dp"]:
        log_dir = "/".join(model_path.split("/")[:-1])
        with open(f"{log_dir}/dataset_stats.pkl", "rb") as f:
            original_dataset_stats = pkl.load(f)
            model_module.set_dataset_stats(original_dataset_stats)

    return model, checkpoint


def load_model_opt_sched(
    model,
    optimizer,
    lr_sched,
    model_path,
    cfg,
    to_load_model=True,
    only_load_model=False,
):
    """
    åœ¨ load_model åŸºç¡€ä¸Šï¼ŒåŠ äº†ä¸€å±‚ç­–ç•¥å¼€å…³
    æ¨¡å‹ + ä¼˜åŒ–å™¨ + è°ƒåº¦å™¨ + epoch å…¨å®¶æ¡¶æ¢å¤â€çš„å·¥å…·å‡½æ•°ã€‚
    Loads a pretrained model from a given path.
    :param model: model to load
    :param optimizer: optimizer to load
    :param lr_sched: learning rate scheduler to load
    :param model_path: path to the pretrained model
    :param cfg: config object
    :param to_load_model: whether to load the model from the checkpoint or not
    """
    if to_load_model:
        model, checkpoint = load_model(model, model_path, cfg)
    else:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)

    if not only_load_model:
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        if lr_sched is not None:
            lr_sched.load_state_dict(checkpoint["lr_sched_state"])

    epoch = checkpoint["epoch"]

    # clean GPU memory
    torch.cuda.empty_cache()
    gc.collect()
    return model, epoch, optimizer, lr_sched


def get_pretrained_model(model_path, device, torch_compile=False):
    """
    Loads a pretrained model from a given path.
    :param model_path: path to the pretrained model
    :param device: device to load the model on, supports only single GPU for now
    :return: model, cfg
    ğŸ‘‰ â€œç»™ä½ ä¸€ä¸ª ckpt è·¯å¾„ï¼Œæˆ‘å¸®ä½ æŠŠ configã€æ¨¡å‹æ„å»ºã€æƒé‡åŠ è½½ã€dataset_stats æ³¨å…¥ã€device è¿ç§»ã€å¯é€‰ compile ä¸€æ¬¡æ€§éƒ½åšå¥½ï¼Œç›´æ¥æ‹¿æ¥æ¨ç†æˆ–å½“é¢„è®­ç»ƒåˆå§‹åŒ–â€ã€‚
    """
    model_folder = "/".join(model_path.split("/")[:-1])
    cfg_path = model_folder + "/config.yaml"
    cfg = get_cfg(cfg_path, cfg_opts="")

    model = get_model(
        cfg, calculate_dataset_stats=False
    )  # don't calculate dataset stats for pretrained model, its loaded from a checkpoint
    # model.to(device) devicemap autoåä¸ç”¨è‡ªå·±ç®¡ç†
    optimizer, lr_sched = get_optimizer(cfg, model, num_gpus=1)

    model, _, _, _ = load_model_opt_sched(
        model=model,
        optimizer=optimizer,
        lr_sched=lr_sched,
        model_path=model_path,
        cfg=cfg,
        only_load_model=True,
    )

    if torch_compile:
        print(
            "Compiling model with torch.compile, this will put the model in eval mode and may take a while..."
        )
        model.eval()
        model = torch.compile(model)
        if hasattr(model, "model"):
            if hasattr(model.model, "generate"):
                print("Compiling model.model.generate with torch.compile")
                model.model.generate = torch.compile(model.model.generate)

    return model, cfg


def get_cfg(cfg_path, cfg_opts):
    # ğŸ‘‰ ä»é»˜è®¤ + é…ç½®æ–‡ä»¶ + å¯é€‰å‘½ä»¤è¡Œè¦†ç›–ï¼Œç”Ÿæˆä¸€ä¸ªâ€œå†»ç»“â€çš„é…ç½®å¯¹è±¡ã€‚
    cfg = get_cfg_defaults()
    if cfg_path != "":
        cfg.merge_from_file(cfg_path)

    if cfg_opts != "":
        cfg.merge_from_list(cfg_opts.split(" "))
        cfg.EXP.EXP_ID += f"_{utils.short_name(cfg_opts)}"
    cfg.freeze()

    print(cfg)
    return cfg


def get_inp(cfg, data_batch):
    """
    Constructs the input for the model using the batched data.
    :param cfg: config object
    :param data_batch: contains the batched data provided by the dataloader
    ç°åœ¨çš„ get_inp æ˜¯â€œç©ºé€‚é…å±‚â€ï¼Œå•¥ä¹Ÿä¸å¹²ï¼Œåªæ˜¯åŸæ ·è¿”å› data_batchï¼Œä½†å®ƒä½œä¸ºæ¥å£å­˜åœ¨ï¼Œæ˜¯ä¸ºäº†ä»¥ååœ¨è¿™é‡Œé›†ä¸­å®ç°ã€Œæ•°æ® batch â†’ æ¨¡å‹è¾“å…¥ã€çš„æ‰€æœ‰è½¬æ¢é€»è¾‘
    """

    inp = data_batch
    return inp


def get_model(cfg, calculate_dataset_stats=True):
    """
    Returns model based on the config
    """
    if cfg.EXP.MODEL == "qwen":
        model = models.QwenActor(**cfg.MODEL.QWEN)
    else:
        assert False, f"Invalid model: {cfg.EXP.MODEL}"

    if calculate_dataset_stats and cfg.EXP.MODEL in ["qwen"]:
        temp_dataset = get_dataloader(split="train", cfg=cfg, get_dataset=True)
        model.set_dataset_stats(temp_dataset.stats)
        del temp_dataset

    return model


def default_batch_proc(data_batch, device):
    for x in data_batch:
        if isinstance(data_batch[x], dict):
            for y in data_batch[x]:
                data_batch[x][y] = data_batch[x][y].to(device).float()
        else:
            if isinstance(data_batch[x], torch.Tensor):
                data_batch[x] = data_batch[x].to(device).float()
            else:
                data_batch[x] = data_batch[x]
    return data_batch


def get_dataloader(split, cfg, get_dataset=False):
    """
    Returns dataloader based on the config and split
    :param get_dataset: whether to return the dataset or the dataloader
    """
    num_workers = cfg.DATALOADER.num_workers
    batch_size = cfg.DATALOADER.batch_size
    dataset_args = {"split": split}

    if cfg.EXP.DATASET == "roboverse":
        print("WARNING: split is ignored for roboverse dataset.")
        dataset_args = dict(**cfg.DATALOADER.ROBOVERSE)
        dataset = roboverse.get_unified_dataset(**dataset_args)
    else:
        raise NotImplementedError

    if "batch_proc" not in dir(dataset):
        dataset.batch_proc = default_batch_proc

    if get_dataset:
        return dataset
    else:
        """
        æ ‡å‡† ImageNet DDP çš„åšæ³•æ˜¯ï¼šç”¨ DistributedSampler æŒ‰ rank åˆ‡åˆ†æ•°æ® â†’ æ¯ä¸ªè¿›ç¨‹çœ‹ ä¸åŒå­é›†ï¼›
        è€Œè¿™ä»½ä»£ç å¦‚æœ roboverse.get_unified_dataset å†…éƒ¨æ²¡æœ‰åš per-rank åˆ‡åˆ†ï¼Œ
        é‚£å°±æ˜¯ï¼šæ¯ä¸ªè¿›ç¨‹éƒ½åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè·‘ä¸€éï¼Œåªæ˜¯é¡ºåºä¸ä¸€æ ·ã€‚
        RoboVerse çš„ dataset å†…éƒ¨è‡ªå·±åšäº† rank/world_size åˆ‡åˆ†?ç­‰æˆ‘çœ‹ä¸‹get_unified_dataset
        """
        return DataLoader(
            dataset,
            batch_size,
            num_workers=num_workers,
            shuffle=(split == "train"),
            drop_last=(split == "train"),
            pin_memory=(torch.cuda.is_available()) and (not num_workers),
            persistent_workers=(num_workers > 0),
        )


def check_grad(model, loss):
    bad_grad = False
    if loss.ne(loss).any():
        bad_grad = True
        print("WARNING: nan in the loss")
    else:
        for x in model.parameters():
            if x.grad is not None:
                if x.grad.ne(x.grad).any():
                    print("WARNING: nan in a gradient")
                    bad_grad = True
                    break
                if ((x.grad == float("inf")) | (x.grad == float("-inf"))).any():
                    print("WARNING: inf in a gradient")
                    bad_grad = True
                    break
    return bad_grad


def get_optimizer(cfg, model, num_gpus=1):
    """
    Returns optimizer and learning rate scheduler based on the config
    :param cfg: config object
    :param model: model to optimize
    :param num_gpus: number of GPUs to optimize the model on, required for scaling the learning rate
    """
    if cfg.EXP.OPTIMIZER == "adam":
        optimizer = torch.optim.Adam(
            model.parameters(), lr=cfg.TRAIN.lr * num_gpus, weight_decay=cfg.TRAIN.l2
        )
    elif cfg.EXP.OPTIMIZER == "adamw":
        optimizer = torch.optim.AdamW(
            model.parameters(), lr=cfg.TRAIN.lr * num_gpus, weight_decay=cfg.TRAIN.l2
        )
    elif cfg.EXP.OPTIMIZER == "adam_bnb":
        import bitsandbytes as bnb

        optimizer = bnb.optim.Adam(
            model.parameters(), lr=cfg.TRAIN.lr * num_gpus, weight_decay=cfg.TRAIN.l2
        )
    elif cfg.EXP.OPTIMIZER == "adamw_bnb":
        import bitsandbytes as bnb

        optimizer = bnb.optim.AdamW(
            model.parameters(), lr=cfg.TRAIN.lr * num_gpus, weight_decay=cfg.TRAIN.l2
        )
    elif cfg.EXP.OPTIMIZER == "adamw_bnb_fp8":
        import bitsandbytes as bnb

        optimizer = bnb.optim.AdamW8bit(
            model.parameters(), lr=cfg.TRAIN.lr * num_gpus, weight_decay=cfg.TRAIN.l2
        )
    else:
        raise NotImplementedError

    if cfg.EXP.LR_SCHED == "none":
        lr_sched = None
    elif cfg.EXP.LR_SCHED == "cosine_anneal":
        lr_sched = lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=cfg.TRAIN.num_epochs, eta_min=cfg.LR_SCHED.lr_clip
        )
    else:
        raise NotImplementedError

    return optimizer, lr_sched


def train(
    cfg,
    loader,
    model,
    optimizer,
    device=0,
    check_grad_fn=False,  # Rename this parameter
    fn_check_time_limit_and_relaunch=None,
    rank=0,
    epoch=0,
    tb=None,
):
    """
    Training for one epoch
    """

    model.train()
    # åˆ›å»ºæ€§èƒ½è·Ÿè¸ªå™¨
    perf = utils.PerfTrackTrain(cfg)

    LOG_EVERY = 20  # æ¯ 20 ä¸ª batch æ‰“ä¸€è¡Œå°
    global_step_base = epoch * len(loader)  # ç”¨äºè®¡ç®—å…¨å±€ step
    TB_EVERY = 1  # æ¯ 1 ä¸ª batch å¾€ TB å†™ä¸€æ¬¡

    time_for = 0  # å‰å‘ä¼ æ’­æ€»æ—¶é—´
    time_bac = 0  # åå‘ä¼ æ’­æ€»æ—¶é—´
    time_dl = 0  # æ•°æ®åŠ è½½æ€»æ—¶é—´
    time4 = time()  # è®°å½•å¾ªç¯å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—æ•°æ®åŠ è½½æ—¶é—´ï¼‰

    epoch_start_time = time()  # ç”¨æ¥ç®—æ¯è¿­ä»£è€—æ—¶
    num_batches = len(loader)  # æ€»å…±å¤šå°‘ä¸ª iter

    # è¿›åº¦æ¡å®½åº¦è‡ªé€‚åº”ç»ˆç«¯å®½åº¦
    # for i, data_batch in tqdm.tqdm(enumerate(loader), dynamic_ncols=True):
    for i, data_batch in enumerate(loader):
        # å¯¹batchæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆå¦‚æ•°æ®å¢å¼ºï¼‰batch processing
        data_batch = loader.dataset.batch_proc(data_batch, device)
        # ä»batchä¸­æå–æ¨¡å‹éœ€è¦çš„è¾“å…¥
        inp = get_inp(cfg, data_batch)

        time1 = time()
        with autocast(device_type="cuda", dtype=torch.bfloat16, enabled=cfg.EXP.AMP):
            out = model(**inp, get_loss=True)
        loss = out["loss"]
        perf.update_all(data_batch=data_batch, out=out, loss=loss)

        time2 = time()  # è®°å½•åå‘ä¼ æ’­å¼€å§‹æ—¶é—´
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œå°†æ¢¯åº¦èŒƒæ•°é™åˆ¶åœ¨cfg.TRAIN.clip_grad_norm
        if cfg.TRAIN.clip_grad_norm != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.TRAIN.clip_grad_norm)

        if check_grad_fn and check_grad(model, loss):  # Use the renamed parameter
            print("WARNING: avoiding step as bad gradient")
        else:
            optimizer.step()

        time3 = time()
        time_dl += time1 - time4
        time_for += time2 - time1
        time_bac += time3 - time2
        time4 = time()
        # =============================================================
        # ====== â­ æ–°å¢ï¼šæ¯ N ä¸ª batch æ‰“å°ä¸€è¡Œ ======
        curr_loss = loss.item()
        avg_loss = perf.agg_loss()
        if rank == 0 and ((i + 1) % LOG_EVERY == 0 or i == 0):
            now = time()
            elapsed = now - epoch_start_time
            it_per_sec = (i + 1) / max(elapsed, 1e-6)

            print(
                f"[epoch {epoch}/{cfg.TRAIN.num_epochs - 1}] "
                f"iter {i+1}/{num_batches}  "
                f"loss {curr_loss:.4f}  avg_loss {avg_loss:.4f}  "
                f"it/s {it_per_sec:.2f}  "
                f"t_fwd {time_for/(i+1):.3f}s  "
                f"t_bwd {time_bac/(i+1):.3f}s  "
                f"t_data {time_dl/(i+1):.3f}s"
            )
        if rank == 0 and tb is not None and (i + 1) % TB_EVERY == 0:
            # ======= â­ å¯é€‰ï¼šå†™å…¥ TensorBoardï¼Œä¸€æ ·åªåœ¨ rank 0 åš =======
            global_step = global_step_base + i + 1
            tb.update(
                "train_iter",
                global_step,
                {"loss": curr_loss, "avg_loss": avg_loss},
            )
        """
        GPU æœåŠ¡å™¨ã€äº‘å¹³å°ã€å­¦æ ¡é›†ç¾¤ï¼ˆSlurm/HTCondorï¼‰éƒ½æœ‰â€œæ—¶é—´é™åˆ¶â€ã€‚
        æ¯”å¦‚ä¸€ä¸ªè®­ç»ƒä»»åŠ¡æœ€å¤šåªèƒ½è·‘ 8 å°æ—¶ï¼Œè¶…è¿‡ä¼šè¢«ç³»ç»Ÿå¼ºåˆ¶æ€æ‰ã€‚
        è®­ç»ƒä¸€æ®µæ—¶é—´ï¼ˆä¾‹å¦‚ 7 å°æ—¶ 50 åˆ†ï¼‰
        å¿«åˆ°æ—¶é—´ä¸Šé™ â†’ è‡ªåŠ¨ä¿å­˜æ¨¡å‹ checkpoint
        ä¼˜é›…é€€å‡ºï¼ˆä¸è®©ç³»ç»Ÿå¼ºåˆ¶æ€ï¼‰
        è‡ªåŠ¨é‡æ–°å¯åŠ¨ç›¸åŒçš„è®­ç»ƒè„šæœ¬
        ä»åˆšåˆšä¿å­˜çš„ checkpoint ç»§ç»­è®­ç»ƒ
        """
        if fn_check_time_limit_and_relaunch is not None:
            # checking every 300 batches ~ 5 minutes
            if (i + 1) % 300 == 0:
                fn_check_time_limit_and_relaunch(perf.agg_loss())

        # uncomment for intermediate printing
        # if i % 10 == 0:
        #     print(f"Iteration {i} time taken: {time_for:.2f}s, {time_bac:.2f}s, {time_dl:.2f}s")

    print(
        f"Avg_loss: {perf.agg_loss():.4f}, "
        f"Forward: {time_for:.2f}s, Backward: {time_bac:.2f}s, "
        f"Data Load: {time_dl:.2f}s, "
        f"Memory Usage: {utils.get_gpu_memory_map()}"
    )

    return perf.agg(), perf.agg_loss()


def print_model_stats(model):
    """Print model statistics including parameter counts."""
    # Get model module if using DDP
    model_module = model.module if isinstance(model, (DDP, FSDP)) else model

    # Count total parameters
    total_params = sum(p.numel() for p in model_module.parameters())

    # Count trainable parameters
    trainable_params = sum(
        p.numel() for p in model_module.parameters() if p.requires_grad
    )

    # Count non-trainable parameters
    non_trainable_params = total_params - trainable_params

    print("=" * 50)
    print("Model Statistics:")
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {non_trainable_params:,}")
    print("=" * 50)


def get_log_dir(cfg, logdir_with_time=False):
    if logdir_with_time:
        log_dir = (
            f"./runs/{cfg.EXP.EXP_ID}/{str(datetime.now())[:-7].replace(' ', '-')}"
        )
    else:
        log_dir = f"./runs/{cfg.EXP.EXP_ID}"
    return log_dir


def entry_train(
    rank,  # å½“å‰è¿›ç¨‹çš„ rankï¼ˆ0,1,2,...ï¼‰ï¼Œç”¨æ¥åš DDPã€å¤šå¡åˆ†å·¥ã€‚
    cfg,
    logdir_with_time=False,
    resume=False,
    model_path="",
    devices=[0],
    port=12345,
):
    """
    Training and evaluating a network based on the specified config.
    """

    # å–å‡ºå½“å‰ rank å¯¹åº”çš„ GPU ID
    device = devices[rank]
    device = f"cuda:{device}"
    # å¦‚æœ devices åˆ—è¡¨é•¿åº¦ > 1ï¼Œå°±è¯´æ˜è¦ç”¨ DDPï¼ˆå¤šå¡ï¼‰ã€‚
    ddp = len(devices) > 1
    # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼ˆdist.init_process_groupï¼‰ã€‚
    # æŠŠæ‰€æœ‰ rank è¿›ç¨‹è¿æˆä¸€æ¡é€šä¿¡çº¿ï¼Œç”¨äºæ¢¯åº¦åŒæ­¥ç­‰ã€‚
    # æ‰€æœ‰å‚ä¸åˆ†å¸ƒå¼è®­ç»ƒçš„è¿›ç¨‹éƒ½è¦å„è‡ªè°ƒç”¨ init_process_group
    utils.setup(rank, world_size=len(devices), port=port)
    torch.cuda.set_device(device)
    if ddp:
        print(f"Running on rank {rank}")

    # ç†è®ºä¸Šæ¯ä¸ª rank ä¼šç”¨ SEED + rank åšéšæœºç§å­ï¼Œä¿è¯åˆ†å¸ƒå¼æ—¶ä¹±æ•°ä¸åŒã€åˆå¯å¤ç°ã€‚
    # ç°åœ¨æ˜¯æ³¨é‡ŠçŠ¶æ€ï¼Œè¯´æ˜ä½œè€…æš‚æ—¶ä¸ç”¨è¿™ä¸ªï¼ˆå¯èƒ½ç»Ÿä¸€åœ¨åˆ«å¤„è®¾ç§å­ï¼‰ã€‚
    # random.seed(cfg.EXP.SEED + rank)
    # np.random.seed(cfg.EXP.SEED + rank)
    # torch.manual_seed(cfg.EXP.SEED + rank)

    loader_train = get_dataloader(split="train", cfg=cfg)
    model = get_model(cfg)
    # model.to(device) # devicemap autoåä¸ç”¨è‡ªå·±ç®¡ç†

    # FSDP å¢åŠ 
    model.device = device

    # é»˜è®¤ to_load_model = Trueï¼Œè¡¨ç¤ºåé¢ä¼šé€šè¿‡ load_model_opt_sched åŠ è½½å‚æ•°
    to_load_model = True
    if (
        hasattr(model, "load_param_before_ddp")
        and model.load_param_before_ddp
        and resume
    ):
        to_load_model = False
        # æœ‰çš„æ¨¡å‹åœ¨åŒ…è¿› DDP å‰åŠ è½½æ›´å®‰å…¨ï¼Œæ¯”å¦‚è‡ªå®šä¹‰ moduleã€å†»ç»“éƒ¨åˆ†å‚æ•°ã€‚
        model, _ = load_model(model, model_path, cfg)
        # device map autoåä¸ç”¨è‡ªå·±ç®¡ç†
        # model.to(device)

    if ddp:
        # Set find_unused_parameters=False when using gradient checkpointing
        # to avoid synchronization issues and deadlocks
        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šä¸€ç§å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç‰ºç‰²è®¡ç®—æ—¶é—´ï¼ˆé‡æ–°è®¡ç®—å‰å‘ä¼ æ’­ï¼‰æ¥å‡å°‘å†…å­˜å ç”¨
        # using_grad_checkpoint = False
        using_grad_checkpoint = True
        if cfg.EXP.MODEL in ["qwen", "qwen_dp"]:
            model_config = (
                cfg.MODEL.QWEN if cfg.EXP.MODEL == "qwen" else cfg.MODEL.QWEN_DP
            )
            # using_grad_checkpoint = getattr(model_config, "grad_checkpoint", False)
            using_grad_checkpoint = getattr(model_config, "grad_checkpoint", True)

        # è¿™æ˜¯åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„å‚æ•°ã€‚
        # é»˜è®¤æƒ…å†µä¸‹ï¼ˆæ— æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰ï¼šå¼€å¯æ£€æµ‹ï¼Œç¡®ä¿è®­ç»ƒæ­£ç¡®æ€§
        # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼šå…³é—­æ£€æµ‹ï¼Œé¿å…ä¸æ£€æŸ¥ç‚¹æœºåˆ¶å†²çª
        find_unused_params = not using_grad_checkpoint
        if rank == 0:
            print(
                f"DDP configuration: grad_checkpoint={using_grad_checkpoint}, find_unused_parameters={find_unused_params}"
            )
        # model = DDP(
        #     model, device_ids=[device], find_unused_parameters=find_unused_params
        # )
        mp_policy = MixedPrecision(
            param_dtype=torch.bfloat16,
            reduce_dtype=torch.bfloat16,
            buffer_dtype=torch.bfloat16,
        )

        # FULL_SHARD: å‚æ•° + æ¢¯åº¦ + optimizer çŠ¶æ€å…¨éƒ¨åˆ‡ç‰‡
        # å¯ç”¨loraæ—¶å€™å´©æºƒï¼ŒFSDPè¦æ±‚ä¸€ä¸ªå¼ é‡é‡Œæ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯ç›¸åŒæ•°æ®ç±»å‹ æš‚æ—¶å…ˆä¸å¤„ç†
        model = FSDP(
            model,
            sharding_strategy=ShardingStrategy.FULL_SHARD,
            backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
            mixed_precision=mp_policy,
            device_id=device,
            use_orig_params=True,
        )
    if rank == 0:
        print(model)

    optimizer, lr_sched = get_optimizer(cfg, model, num_gpus=len(devices))
    if resume:
        model, old_epoch, optimizer, lr_sched = load_model_opt_sched(
            model=model,
            optimizer=optimizer,
            lr_sched=lr_sched,
            model_path=model_path,
            cfg=cfg,
            to_load_model=to_load_model,
        )
    else:
        assert model_path == "", model_path
        old_epoch = -1

    if rank == 0:
        print_model_stats(model)

    # æ‰€æœ‰ rank åœ¨è¿™é‡Œ é›†åˆ ä¸€æ¬¡ã€‚
    # ç¡®ä¿æ¨¡å‹åŠ è½½ã€optimizer åˆå§‹åŒ–ã€log_dir å‡†å¤‡ç­‰éƒ½å®Œæˆäº†ï¼Œå†ä¸€èµ·è¿›å…¥è®­ç»ƒå¾ªç¯ã€‚
    dist.barrier()

    # æ—¥å¿—ç›®å½• & TensorBoard åªç”± rank 0 ç®¡
    if rank == 0:
        log_dir = get_log_dir(cfg, logdir_with_time)
        print(f"Log directory: {log_dir}")
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        with open(f"{log_dir}/config.yaml", "w") as f:
            with redirect_stdout(f):
                print(cfg.dump())

        # tb is initialized only for rank 0
        tb = utils.TensorboardManager(log_dir)
    else:
        # log_dir and tb should not be used for any rank other than rank 0
        log_dir = ""
        tb = None
    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(old_epoch + 1, cfg.TRAIN.num_epochs):
        # fn_check_time_limit_and_relaunch ç›®å‰æ˜¯ Noneï¼Œé¢„ç•™â€œè¶…æ—¶é‡å¯â€é’©å­ï¼ˆæ¯”å¦‚é›†ç¾¤ job é™æ—¶ï¼‰ã€‚
        fn_check_time_limit_and_relaunch = None

        # print epoch number
        if rank == 0:
            print(f"Training for epoch {epoch} / {cfg.TRAIN.num_epochs}")

        # train
        train_perf, train_loss = train(
            cfg=cfg,
            loader=loader_train,
            model=model,
            optimizer=optimizer,
            device=device,
            fn_check_time_limit_and_relaunch=fn_check_time_limit_and_relaunch,
            rank=rank,
            epoch=epoch,
            tb=tb if rank == 0 else None,
        )

        # update tensorboard
        if rank == 0:
            _lr = (
                lr_sched.optimizer.param_groups[0]["lr"]
                if lr_sched
                else optimizer.param_groups[0]["lr"]
            )
            pprint.pprint(f"Performance: {train_perf}", width=80)
            tb.update("train", epoch, train_perf)
            tb.update(
                "train",
                epoch,
                {"loss": train_loss, "lr": _lr},
            )

        # save checkpoint
        if rank == 0:
            if not (cfg.EXP_EXTRA.save_ckp == 0) and (
                epoch % cfg.EXP_EXTRA.save_ckp == 0
            ):
                save_checkpoint(
                    f"{epoch}",
                    epoch,
                    model,
                    optimizer,
                    lr_sched,
                    cfg,
                    log_dir,
                )

            if cfg.EXP_EXTRA.save_last_ckpt:
                # change name of last checkpoint to second_last so that it is not overwritten by the new last checkpoint.
                # this second last checkpoint will be used to resume training if the training is relaunched because of loss increase
                if os.path.exists(log_dir + "/model_last.pth"):
                    # remove second last checkpoint if it exists
                    if os.path.exists(log_dir + "/model_second_last.pth"):
                        os.remove(log_dir + "/model_second_last.pth")
                    os.rename(
                        log_dir + "/model_last.pth", log_dir + "/model_second_last.pth"
                    )
                if os.path.exists(log_dir + "/model_last"):
                    # remove second last checkpoint if it exists
                    if os.path.exists(log_dir + "/model_second_last"):
                        shutil.rmtree(log_dir + "/model_second_last")
                    os.rename(log_dir + "/model_last", log_dir + "/model_second_last")
                save_checkpoint(
                    "last",
                    epoch,
                    model,
                    optimizer,
                    lr_sched,
                    cfg,
                    log_dir,
                )

        # update learning rate
        if cfg.EXP.LR_SCHED in ["none"]:
            print(f"Current lr: {optimizer.param_groups[0]['lr']}")
        elif cfg.EXP.LR_SCHED in ["cosine_anneal"]:
            lr_sched.step()
            print(f"Current lr: {lr_sched.optimizer.param_groups[0]['lr']}")
        else:
            raise NotImplementedError

    if rank == 0:
        print("Saving the final model")
        save_checkpoint(
            "final",
            cfg.TRAIN.num_epochs - 1,
            model,
            optimizer,
            lr_sched,
            cfg,
            log_dir,
        )

    if rank == 0:
        # close tensorboard
        tb.close()


if __name__ == "__main__":
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if DEVICE.type == "cpu":
        print("WARNING: Using CPU")

    parser = argparse.ArgumentParser()
    parser.set_defaults(entry=lambda cmd_args: parser.print_help())
    parser.add_argument("--entry", type=str, default="train")
    parser.add_argument("--exp-config", type=str, default="")
    parser.add_argument("--exp-cfg-opts", type=str, default="")
    parser.add_argument("--model-path", type=str, default="")
    parser.add_argument("--logdir-with-time", action="store_true", default=False)
    parser.add_argument("--resume", action="store_true", default=False)
    parser.add_argument("--devices", type=str, default="0")

    cmd_args = parser.parse_args()

    if cmd_args.entry == "train":
        assert (
            not cmd_args.logdir_with_time
        ), "Temporarily disable logdir_with_time as it is not handled properly when autoresuming and auto-relaunching with loss increase. It is fine for one time launch or manual relaunching."
        _cfg = get_cfg(cmd_args.exp_config, cmd_args.exp_cfg_opts)
        if cmd_args.resume:
            if cmd_args.model_path == "":
                print(
                    "WARNING: No model path provided, resuming from latest checkpoint"
                )
                log_dir = get_log_dir(_cfg, cmd_args.logdir_with_time)
                cmd_args.model_path = os.path.join(log_dir, "model_last.pth")
            print(f"Resuming from {cmd_args.model_path}")
        else:
            assert cmd_args.model_path == ""

        devices = cmd_args.devices.split(",")
        devices = [int(x) for x in devices]
        # éšæœºåœ¨ [27000, 29999] èŒƒå›´é‡Œé€‰ä¸€ä¸ªç«¯å£å·ï¼Œç”¨äº DDP è¿›ç¨‹é—´é€šä¿¡ï¼ˆinit_process_groupï¼‰ã€‚
        port = (random.randint(0, 3000) % 3000) + 27000
        mp.spawn(
            entry_train,
            args=(
                _cfg,
                cmd_args.logdir_with_time,
                cmd_args.resume,
                cmd_args.model_path,
                devices,
                port,
            ),
            nprocs=len(devices),
            join=True,
        )

    else:
        assert False
